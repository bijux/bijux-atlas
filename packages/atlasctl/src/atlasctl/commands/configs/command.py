from __future__ import annotations

import argparse
import difflib
import hashlib
import json
import re
from datetime import date

try:
    import tomllib  # type: ignore[attr-defined]
except ModuleNotFoundError:  # pragma: no cover - python<3.11
    tomllib = None  # type: ignore[assignment]
from pathlib import Path

import jsonschema

from ...core.context import RunContext
from ...core.exec import run
from ...core.fs import ensure_evidence_path
from ...core.runtime.paths import write_text_file
from ...core.runtime.tooling import read_pins, read_tool_versions

CONFIG_SCHEMA_PAIRS: tuple[tuple[str, str], ...] = (
    ("configs/inventory/owners.json", "configs/schema/configs-ownership.schema.json"),
    ("configs/ops/tool-versions.json", "configs/schema/tool-versions.schema.json"),
    ("configs/ops/public-surface.json", "configs/schema/public-surface.schema.json"),
    ("configs/policy/policy-relaxations.json", "configs/schema/policy-relaxations.schema.json"),
    ("configs/policy/layer-relaxations.json", "configs/schema/layer-relaxations.schema.json"),
    ("configs/policy/ops-lint-relaxations.json", "configs/schema/ops-lint-relaxations.schema.json"),
    ("configs/policy/layer-live-diff-allowlist.json", "configs/schema/layer-live-diff-allowlist.schema.json"),
    ("configs/ops/target-renames.json", "configs/schema/target-renames.schema.json"),
    ("configs/ops/hpa-safety-caps.json", "configs/schema/hpa-safety-caps.schema.json"),
    ("configs/meta/ownership.json", "configs/schema/meta-ownership.schema.json"),
)
CANONICAL_INVENTORY_ROOT = "ops/inventory"
CONFIG_COMPILER_GENERATED_DIR = "configs/_generated"
GENERATED_HEADER = "# GENERATED BY atlasctl configs gen. DO NOT EDIT.\n"
CONFIG_COMPILER_REPORT_SCHEMA = "configs/schema/config-compiler-report.schema.json"

SKIP_PARTS = {"schema", "__pycache__", ".vale"}
_CONFIGS_ITEMS: tuple[str, ...] = ("diff", "drift", "fmt", "gen", "generate", "print", "sync", "validate")


def normalize_config_key(raw: str) -> str:
    value = raw.strip().replace("-", "_").upper()
    if not re.fullmatch(r"[A-Z][A-Z0-9_]*", value):
        raise ValueError(f"invalid config key: {raw}")
    return value


def _load_json(path: Path) -> dict[str, object]:
    return json.loads(path.read_text(encoding="utf-8"))


def _parse_yaml(path: Path) -> str | None:
    proc = run(
        [
            "python3",
            "-c",
            "import sys,yaml; yaml.safe_load(open(sys.argv[1], 'r', encoding='utf-8').read())",
            str(path),
        ],
        text=True,
        capture_output=True,
    )
    if proc.returncode == 0:
        return None
    return proc.stderr.strip() or proc.stdout.strip() or "yaml parse failed"


def _collect_config_files(repo_root: Path) -> list[Path]:
    out: list[Path] = []
    for p in sorted((repo_root / "configs").rglob("*")):
        if not p.is_file():
            continue
        rel = p.relative_to(repo_root)
        if any(part in SKIP_PARTS for part in rel.parts):
            continue
        out.append(p)
    return out


def _validate_schema_pairs(repo_root: Path) -> list[str]:
    errors: list[str] = []
    for data_rel, schema_rel in CONFIG_SCHEMA_PAIRS:
        try:
            data = _load_json(repo_root / data_rel)
            schema = _load_json(repo_root / schema_rel)
            jsonschema.validate(data, schema)
        except Exception as exc:  # noqa: BLE001
            errors.append(f"{data_rel} vs {schema_rel}: {exc}")
    return errors


def _validate_file_well_formed(repo_root: Path) -> list[str]:
    errors: list[str] = []
    for p in _collect_config_files(repo_root):
        rel = p.relative_to(repo_root).as_posix()
        if p.suffix == ".json":
            try:
                json.loads(p.read_text(encoding="utf-8"))
            except Exception as exc:  # noqa: BLE001
                errors.append(f"{rel}: invalid json ({exc})")
        elif p.suffix in {".yaml", ".yml"}:
            err = _parse_yaml(p)
            if err:
                errors.append(f"{rel}: invalid yaml ({err})")
        elif p.suffix == ".toml":
            if tomllib is None:
                continue
            try:
                tomllib.loads(p.read_text(encoding="utf-8"))
            except Exception as exc:  # noqa: BLE001
                errors.append(f"{rel}: invalid toml ({exc})")
    return errors


def _check_ownership(repo_root: Path) -> list[str]:
    own = _load_json(repo_root / "configs/inventory/owners.json")
    areas = {str(k) for k in own.get("areas", {}).keys()}
    errors: list[str] = []
    for d in sorted((repo_root / "configs").iterdir()):
        if not d.is_dir() or d.name.startswith("_"):
            continue
        rel = f"configs/{d.name}"
        if rel not in areas:
            errors.append(f"missing ownership mapping: {rel}")
    for area in sorted(areas):
        if not (repo_root / area).exists():
            errors.append(f"ownership points to missing area: {area}")
    return errors


def _check_keys_docs_coverage(repo_root: Path) -> list[str]:
    registry_payload = _load_json(repo_root / "docs/contracts/CONFIG_KEYS.json")
    docs = (repo_root / "docs/contracts/config-keys.md").read_text(encoding="utf-8")
    internal = {
        ln.strip()
        for ln in (repo_root / "configs/ops/internal-config-keys.txt").read_text(encoding="utf-8").splitlines()
        if ln.strip() and not ln.strip().startswith("#")
    }
    keys = {str(k) for k in registry_payload.get("env_keys", []) if isinstance(k, str)}
    missing = sorted(k for k in keys if k not in internal and f"`{k}`" not in docs)
    return [f"missing docs coverage for key: {k}" for k in missing]


def _check_generated_drift(repo_root: Path) -> list[str]:
    expected = _generate_outputs(repo_root, write=False)
    errors: list[str] = []
    for rel, text in expected.items():
        path = repo_root / rel
        if not path.exists() or path.read_text(encoding="utf-8") != text:
            errors.append(f"generated drift: {rel}")
    return errors


def _check_public_surface_doc_exceptions_expiry(repo_root: Path) -> list[str]:
    path = repo_root / "configs/ops/public-surface-doc-exceptions.txt"
    if not path.exists():
        return []
    today = date.today()
    errors: list[str] = []
    for idx, raw in enumerate(path.read_text(encoding="utf-8").splitlines(), start=1):
        line = raw.strip()
        if not line or line.startswith("#"):
            continue
        if "expires=" not in line:
            errors.append(f"{path.relative_to(repo_root).as_posix()}:{idx}: missing expires=YYYY-MM-DD")
            continue
        exp_raw = line.split("expires=", 1)[1].strip()
        try:
            exp = date.fromisoformat(exp_raw)
        except ValueError:
            errors.append(f"{path.relative_to(repo_root).as_posix()}:{idx}: invalid expires date `{exp_raw}`")
            continue
        if exp < today:
            errors.append(f"{path.relative_to(repo_root).as_posix()}:{idx}: expired on {exp.isoformat()}")
    return errors


def _overlay_paths(repo_root: Path) -> list[Path]:
    root = repo_root / "ops" / "env" / "overlays"
    if not root.exists():
        return []
    return sorted(p for p in root.rglob("*") if p.is_file() and p.suffix in {".json", ".yaml", ".yml"})


def _overlay_merge_models(repo_root: Path) -> tuple[list[dict[str, object]], list[str]]:
    base = _load_json(repo_root / "configs/ops/env.schema.json")
    base_vars = base.get("variables", {}) if isinstance(base, dict) else {}
    allowed_override_fields = {"default", "description"}
    rows: list[dict[str, object]] = []
    errors: list[str] = []
    owners_by_var: dict[str, str] = {}
    for path in _overlay_paths(repo_root):
        rel = path.relative_to(repo_root).as_posix()
        raw: object
        if path.suffix == ".json":
            raw = json.loads(path.read_text(encoding="utf-8"))
        else:
            raw = _parse_yaml_doc(path)
        payload = raw if isinstance(raw, dict) else {}
        vars_map = payload.get("variables", {}) if isinstance(payload, dict) else {}
        if not isinstance(vars_map, dict):
            errors.append(f"{rel}: variables must be object")
            continue
        overrides = 0
        for key, spec in vars_map.items():
            if key not in base_vars:
                errors.append(f"{rel}: variable `{key}` not in base schema")
                continue
            if not isinstance(spec, dict):
                errors.append(f"{rel}: variable `{key}` override must be object")
                continue
            illegal = sorted(set(spec.keys()) - allowed_override_fields)
            if illegal:
                errors.append(f"{rel}: variable `{key}` illegal override fields: {', '.join(illegal)}")
            prior = owners_by_var.get(str(key))
            if prior is not None and prior != rel:
                errors.append(f"{rel}: variable `{key}` shadows override in {prior}")
            owners_by_var[str(key)] = rel
            overrides += 1
        rows.append({"path": rel, "variables": sorted(str(k) for k in vars_map.keys()), "override_count": overrides})
    return rows, errors


def _parse_yaml_doc(path: Path) -> object:
    try:
        import yaml  # type: ignore
    except Exception as exc:  # pragma: no cover
        raise RuntimeError("PyYAML is required for config yaml parsing") from exc
    return yaml.safe_load(path.read_text(encoding="utf-8"))


def _generate_outputs(repo_root: Path, write: bool) -> dict[str, str]:
    own = _load_json(repo_root / "configs/inventory/owners.json").get("areas", {})
    lines_index = ["# Configs Index", "", "Canonical configuration surface for repository behavior.", "", "## Areas"]
    for area in sorted(own):
        lines_index.append(f"- `{area}` owner: `{own[area]}`")
    lines_index += ["", "See also: `configs/inventory/owners.json`.", ""]
    configs_index = "\n".join(lines_index)

    lines_surface = ["# Configs Surface", "", "Generated from `configs/` structure and ownership map.", ""]
    for d in sorted((repo_root / "configs").iterdir()):
        if not d.is_dir() or d.name.startswith("_"):
            continue
        rel = f"configs/{d.name}"
        owner = own.get(rel, "<unowned>")
        readme = d / "README.md"
        lines_surface.append(f"## `{rel}`")
        lines_surface.append(f"- Owner: `{owner}`")
        lines_surface.append(f"- README: `{readme.relative_to(repo_root).as_posix() if readme.exists() else 'MISSING'}`")
        lines_surface.append("")
    configs_surface = "\n".join(lines_surface)

    reg = _load_json(repo_root / "docs/contracts/CONFIG_KEYS.json")
    env_keys = sorted({str(k) for k in reg.get("env_keys", []) if isinstance(k, str) and str(k).startswith(("ATLAS_", "BIJUX_"))})
    env_keys.append("ATLAS_DEV_ALLOW_UNKNOWN_ENV")
    env_contract = json.dumps(
        {
            "schema_version": 1,
            "description": "Runtime env allowlist contract for atlas-server; unknown ATLAS_/BIJUX_ keys are rejected unless dev escape hatch is enabled.",
            "enforced_prefixes": ["ATLAS_", "BIJUX_"],
            "dev_mode_allow_unknown_env": "ATLAS_DEV_ALLOW_UNKNOWN_ENV",
            "allowed_env": sorted(set(env_keys)),
        },
        indent=2,
        sort_keys=True,
    ) + "\n"

    tooling = read_tool_versions(repo_root)
    lines_tooling = ["# Tooling Versions", "", "Generated from `configs/ops/tool-versions.json`.", ""]
    for k, v in sorted(tooling.items()):
        lines_tooling.append(f"- `{k}`: `{v}`")
    lines_tooling.append("")
    tooling_doc = "\n".join(lines_tooling)

    outputs = {
        "configs/INDEX.md": configs_index,
        "docs/_generated/configs-surface.md": configs_surface,
        "configs/contracts/env.schema.json": env_contract,
        "docs/_generated/tooling-versions.md": tooling_doc,
    }
    overlay_rows, overlay_errors = _overlay_merge_models(repo_root)
    compiler_report = json.dumps(
        {
            "schema_version": 1,
            "kind": "atlasctl-config-compiler.generated",
            "inventory_root": CANONICAL_INVENTORY_ROOT,
            "generated_root": CONFIG_COMPILER_GENERATED_DIR,
            "overlay_model": {
                "base": "configs/ops/env.schema.json",
                "env_dir": "ops/env/overlays",
                "whitelist_override_fields": ["default", "description"],
                "overlays": overlay_rows,
                "errors": overlay_errors,
            },
            "locks": {
                "toolchain": "ops/inventory/toolchain.yaml",
                "helm_versions": "configs/ops/pins/helm.json",
                "pinned_queries": "ops/load/queries/pinned-v1.lock",
                "tool_versions_ssot": "configs/ops/tool-versions.json",
            },
        },
        indent=2,
        sort_keys=True,
    ) + "\n"
    compiler_index = "\n".join(
        [
            GENERATED_HEADER.strip(),
            "",
            "# Config Compiler Outputs",
            "",
            f"- Inventory root (authoritative): `{CANONICAL_INVENTORY_ROOT}`",
            f"- Generated root: `{CONFIG_COMPILER_GENERATED_DIR}`",
            "- Do not reference files in this directory directly from docs/make/workflows; use `atlasctl configs ...` commands.",
            "",
        ]
    ) + "\n"
    outputs[f"{CONFIG_COMPILER_GENERATED_DIR}/INDEX.md"] = compiler_index
    outputs[f"{CONFIG_COMPILER_GENERATED_DIR}/compiler-report.json"] = GENERATED_HEADER + compiler_report
    checksums_payload = {
        "schema_version": 1,
        "kind": "atlasctl-config-generated-checksums",
        "generated_root": CONFIG_COMPILER_GENERATED_DIR,
        "files": [],
    }
    for rel, text in sorted(outputs.items()):
        if not rel.startswith(f"{CONFIG_COMPILER_GENERATED_DIR}/"):
            continue
        body = text
        if rel.endswith(".json") and not body.startswith(GENERATED_HEADER):
            body = GENERATED_HEADER + body
            outputs[rel] = body
        checksums_payload["files"].append(
            {"path": rel, "sha256": hashlib.sha256(body.encode("utf-8")).hexdigest()}
        )
    outputs[f"{CONFIG_COMPILER_GENERATED_DIR}/checksums.json"] = GENERATED_HEADER + json.dumps(checksums_payload, indent=2, sort_keys=True) + "\n"
    compiler_report_payload = json.loads(outputs[f"{CONFIG_COMPILER_GENERATED_DIR}/compiler-report.json"].split("\n", 1)[1])
    compiler_report_schema = _load_json(repo_root / CONFIG_COMPILER_REPORT_SCHEMA)
    jsonschema.validate(compiler_report_payload, compiler_report_schema)
    if write:
        for rel, text in outputs.items():
            out = repo_root / rel
            write_text_file(out, text, encoding="utf-8")
    return outputs


def _config_compiler_validate(repo_root: Path) -> tuple[list[str], dict[str, str]]:
    errors = []
    errors.extend(_validate_schema_pairs(repo_root))
    errors.extend(_validate_file_well_formed(repo_root))
    errors.extend(_check_ownership(repo_root))
    errors.extend(_check_keys_docs_coverage(repo_root))
    ok_sync, _ = _sync_slo(repo_root, write=False)
    if not ok_sync:
        errors.append("SLO sync drift: configs/slo/slo.json")
    overlay_rows, overlay_errors = _overlay_merge_models(repo_root)
    _ = overlay_rows
    errors.extend(overlay_errors)
    lock_errors = _check_lock_discipline(repo_root)
    errors.extend(lock_errors)
    errors.extend(_check_generated_checksums_strict(repo_root))
    errors.extend(_check_compiler_report_schema(repo_root))
    errors.extend(_check_tool_versions_ssot(repo_root))
    errors.extend(_check_public_surface_doc_exceptions_expiry(repo_root))
    checks = {
        "schema_pairs": "pass" if not _validate_schema_pairs(repo_root) else "fail",
        "well_formed": "pass" if not _validate_file_well_formed(repo_root) else "fail",
        "ownership": "pass" if not _check_ownership(repo_root) else "fail",
        "keys_docs": "pass" if not _check_keys_docs_coverage(repo_root) else "fail",
        "slo_sync": "pass" if ok_sync else "fail",
        "overlay_merge_model": "pass" if not overlay_errors else "fail",
        "lock_discipline": "pass" if not lock_errors else "fail",
        "generated_checksums": "pass" if not _check_generated_checksums_strict(repo_root) else "fail",
        "compiler_report_schema": "pass" if not _check_compiler_report_schema(repo_root) else "fail",
        "tool_versions_ssot": "pass" if not _check_tool_versions_ssot(repo_root) else "fail",
        "public_surface_doc_exceptions_expiry": "pass" if not _check_public_surface_doc_exceptions_expiry(repo_root) else "fail",
    }
    return errors, checks


def _check_lock_discipline(repo_root: Path) -> list[str]:
    errors: list[str] = []
    required = [
        "ops/inventory/toolchain.yaml",
        "configs/ops/pins/helm.json",
        "ops/load/queries/pinned-v1.lock",
    ]
    for rel in required:
        if not (repo_root / rel).exists():
            errors.append(f"missing lock artifact: {rel}")
    return errors


def _check_tool_versions_ssot(repo_root: Path) -> list[str]:
    errors: list[str] = []
    canonical = repo_root / "configs/ops/tool-versions.json"
    if not canonical.exists():
        return ["missing SSOT tool versions file: configs/ops/tool-versions.json"]
    for path in sorted((repo_root / "configs").rglob("*tool*version*.json")):
        rel = path.relative_to(repo_root).as_posix()
        if rel == "configs/ops/tool-versions.json":
            continue
        if rel.startswith("configs/_generated/"):
            continue
        errors.append(f"non-SSOT tool versions file detected: {rel}")
    return errors


def _check_generated_checksums_strict(repo_root: Path) -> list[str]:
    generated_root = repo_root / CONFIG_COMPILER_GENERATED_DIR
    checksums_path = generated_root / "checksums.json"
    if not generated_root.exists():
        return [f"{CONFIG_COMPILER_GENERATED_DIR} missing"]
    if not checksums_path.exists():
        return [f"{CONFIG_COMPILER_GENERATED_DIR}/checksums.json missing"]
    text = checksums_path.read_text(encoding="utf-8")
    if not text.startswith(GENERATED_HEADER):
        return [f"{CONFIG_COMPILER_GENERATED_DIR}/checksums.json missing generated header"]
    payload = json.loads(text.split("\n", 1)[1])
    files = payload.get("files", [])
    if not isinstance(files, list):
        return [f"{CONFIG_COMPILER_GENERATED_DIR}/checksums.json files must be a list"]
    declared = {}
    errors: list[str] = []
    for row in files:
        if not isinstance(row, dict):
            errors.append("checksums files entry must be object")
            continue
        rel = str(row.get("path", ""))
        sha = str(row.get("sha256", ""))
        if not rel.startswith(f"{CONFIG_COMPILER_GENERATED_DIR}/"):
            errors.append(f"checksums path outside generated root: {rel}")
            continue
        if rel in declared:
            errors.append(f"duplicate checksum entry: {rel}")
            continue
        declared[rel] = sha
    for path in sorted(generated_root.rglob("*")):
        if not path.is_file():
            continue
        rel = path.relative_to(repo_root).as_posix()
        if rel == f"{CONFIG_COMPILER_GENERATED_DIR}/checksums.json":
            continue
        body = path.read_text(encoding="utf-8")
        if not body.startswith(GENERATED_HEADER):
            errors.append(f"{rel}: missing generated header")
        if rel not in declared:
            errors.append(f"missing checksum entry: {rel}")
            continue
        digest = hashlib.sha256(body.encode("utf-8")).hexdigest()
        if declared[rel] != digest:
            errors.append(f"{rel}: checksum mismatch")
    for rel in sorted(declared):
        if not (repo_root / rel).exists():
            errors.append(f"checksum entry points to missing file: {rel}")
    return errors


def _check_compiler_report_schema(repo_root: Path) -> list[str]:
    path = repo_root / CONFIG_COMPILER_GENERATED_DIR / "compiler-report.json"
    schema_path = repo_root / CONFIG_COMPILER_REPORT_SCHEMA
    if not path.exists():
        return [f"{CONFIG_COMPILER_GENERATED_DIR}/compiler-report.json missing"]
    if not schema_path.exists():
        return [f"{CONFIG_COMPILER_REPORT_SCHEMA} missing"]
    try:
        payload = json.loads(path.read_text(encoding="utf-8").split("\n", 1)[1])
        schema = _load_json(schema_path)
        jsonschema.validate(payload, schema)
    except Exception as exc:  # noqa: BLE001
        return [f"compiler report schema validation failed: {exc}"]
    return []


def _config_fmt(repo_root: Path, *, check_only: bool) -> list[str]:
    touched: list[str] = []
    for p in _collect_config_files(repo_root):
        rel = p.relative_to(repo_root).as_posix()
        raw = p.read_text(encoding="utf-8")
        formatted: str | None = None
        if p.suffix == ".json":
            try:
                payload = json.loads(raw)
                formatted = json.dumps(payload, indent=2, sort_keys=True) + "\n"
            except Exception:
                formatted = None
        elif p.suffix in {".yaml", ".yml"}:
            try:
                import yaml  # type: ignore

                payload = yaml.safe_load(raw)
                formatted = yaml.safe_dump(payload, sort_keys=True, allow_unicode=False)
            except Exception:
                formatted = None
        if formatted is None:
            continue
        if raw != formatted:
            touched.append(rel)
            if not check_only:
                write_text_file(p, formatted, encoding="utf-8")
    return touched


def _config_diff_payload(repo_root: Path) -> tuple[list[str], dict[str, str]]:
    expected = _generate_outputs(repo_root, write=False)
    diffs: list[str] = []
    for rel, text in sorted(expected.items()):
        path = repo_root / rel
        current = path.read_text(encoding="utf-8") if path.exists() else ""
        if current == text:
            continue
        diff = "".join(
            difflib.unified_diff(
                current.splitlines(keepends=True),
                text.splitlines(keepends=True),
                fromfile=f"a/{rel}",
                tofile=f"b/{rel}",
            )
        ).strip()
        diffs.append(diff or f"drift: {rel}")
    return diffs, {"generated_root": CONFIG_COMPILER_GENERATED_DIR}


def _sync_slo(repo_root: Path, write: bool) -> tuple[bool, str]:
    src = _load_json(repo_root / "configs/ops/slo/slo.v1.json")
    expected = {
        "schema_version": src.get("schema_version", 1),
        "source": "configs/ops/slo/slo.v1.json",
        "slis": src.get("slis", []),
        "slos": src.get("slos", []),
        "change_policy": src.get("change_policy", {}),
    }
    dst = repo_root / "configs/slo/slo.json"
    if write:
        write_text_file(dst, json.dumps(expected, indent=2, sort_keys=True) + "\n", encoding="utf-8")
        return True, "configs/slo/slo.json"
    current = _load_json(dst)
    return current == expected, "configs/slo/slo.json"


def _print_payload(repo_root: Path) -> dict[str, object]:
    sources = [
        "configs/policy/policy.json",
        "configs/ops/env.schema.json",
        "configs/ops/tool-versions.json",
        "configs/ops/observability-pack.json",
        "configs/perf/k6-thresholds.v1.json",
        "configs/slo/slo.json",
    ]
    provenance = []
    for src in sources:
        raw = (repo_root / src).read_bytes()
        provenance.append({"path": src, "sha256": hashlib.sha256(raw).hexdigest()})
    return {
        "policy": _load_json(repo_root / "configs/policy/policy.json"),
        "ops_env_schema": _load_json(repo_root / "configs/ops/env.schema.json"),
        "ops_tool_versions": _load_json(repo_root / "configs/ops/tool-versions.json"),
        "ops_pins": read_pins(repo_root),
        "ops_observability_pack": _load_json(repo_root / "configs/ops/observability-pack.json"),
        "perf_thresholds": _load_json(repo_root / "configs/perf/k6-thresholds.v1.json"),
        "slo": _load_json(repo_root / "configs/slo/slo.json"),
        "_provenance": provenance,
    }


def _emit(payload: dict[str, object], report: str) -> None:
    print(json.dumps(payload, sort_keys=True) if report == "json" else json.dumps(payload, indent=2, sort_keys=True))


def run_configs_command(ctx: RunContext, ns: argparse.Namespace) -> int:
    report = getattr(ns, "report", "text")
    repo = ctx.repo_root
    if not getattr(ns, "configs_cmd", None) and bool(getattr(ns, "list", False)):
        if bool(getattr(ns, "json", False)):
            _emit({"schema_version": 1, "tool": "atlasctl", "status": "ok", "group": "configs", "items": list(_CONFIGS_ITEMS)}, "json")
        else:
            for item in _CONFIGS_ITEMS:
                print(item)
        return 0

    if ns.configs_cmd == "print":
        _emit({"schema_version": 1, "tool": "atlasctl", "status": "pass", "payload": _print_payload(repo)}, report)
        return 0

    if ns.configs_cmd == "validate":
        errors, checks = _config_compiler_validate(repo)
        ok_sync = checks.get("slo_sync") == "pass"
        payload = {
            "schema_version": 1,
            "tool": "atlasctl",
            "kind": "atlasctl-config-validate",
            "inventory_root": CANONICAL_INVENTORY_ROOT,
            "status": "pass" if not errors else "fail",
            "checks": checks,
            "errors": errors,
        }
        if getattr(ns, "emit_artifacts", False):
            out = ensure_evidence_path(ctx, repo / "artifacts/evidence/configs/validate" / ctx.run_id / "report.json")
            write_text_file(out, json.dumps(payload, indent=2, sort_keys=True) + "\n", encoding="utf-8")
        _emit(payload, report)
        return 0 if not errors else 1

    if ns.configs_cmd == "generate":
        _generate_outputs(repo, write=True)
        payload = {
            "schema_version": 1,
            "tool": "atlasctl",
            "status": "pass",
            "outputs": sorted(_generate_outputs(repo, write=False).keys()),
        }
        if getattr(ns, "check", False):
            drift = _check_generated_drift(repo)
            payload["status"] = "pass" if not drift else "fail"
            payload["errors"] = drift
            _emit(payload, report)
            return 0 if not drift else 1
        _emit(payload, report)
        return 0
    if ns.configs_cmd == "gen":
        _generate_outputs(repo, write=True)
        payload = {"schema_version": 1, "tool": "atlasctl", "kind": "atlasctl-config-gen", "status": "pass", "generated_root": CONFIG_COMPILER_GENERATED_DIR, "outputs": sorted(_generate_outputs(repo, write=False).keys())}
        if bool(getattr(ns, "check", False)):
            diffs, _meta = _config_diff_payload(repo)
            payload["status"] = "pass" if not diffs else "fail"
            payload["diff_count"] = len(diffs)
            payload["diffs"] = diffs
            _emit(payload, report)
            return 0 if not diffs else 1
        _emit(payload, report)
        return 0
    if ns.configs_cmd == "diff":
        diffs, meta = _config_diff_payload(repo)
        fail = bool(getattr(ns, "fail", False))
        payload = {"schema_version": 1, "tool": "atlasctl", "kind": "atlasctl-config-diff", "status": "pass" if not diffs else "fail", **meta, "diff_count": len(diffs), "diffs": diffs}
        _emit(payload, report)
        return 0 if (not diffs or not fail) else 1
    if ns.configs_cmd == "fmt":
        changed = _config_fmt(repo, check_only=bool(getattr(ns, "check", False)))
        payload = {"schema_version": 1, "tool": "atlasctl", "kind": "atlasctl-config-fmt", "status": "pass" if not (changed and getattr(ns, 'check', False)) else "fail", "check_only": bool(getattr(ns, "check", False)), "changed": changed}
        _emit(payload, report)
        return 0 if not (changed and getattr(ns, "check", False)) else 1

    if ns.configs_cmd == "sync":
        ok, target = _sync_slo(repo, write=getattr(ns, "write", False))
        payload = {"schema_version": 1, "tool": "atlasctl", "status": "pass" if ok else "fail", "target": target}
        _emit(payload, report)
        return 0 if ok else 1

    if ns.configs_cmd == "drift":
        drift = _check_generated_drift(repo)
        payload = {"schema_version": 1, "tool": "atlasctl", "status": "pass" if not drift else "fail", "errors": drift}
        _emit(payload, report)
        return 0 if not drift else 1

    return 2


def configure_configs_parser(sub: argparse._SubParsersAction[argparse.ArgumentParser]) -> None:
    p = sub.add_parser("configs", help="native configs validation/generation/sync commands")
    p.add_argument("--list", action="store_true", help="list available configs commands")
    p.add_argument("--json", action="store_true", help="emit machine-readable JSON output")
    cfg = p.add_subparsers(dest="configs_cmd", required=False)

    c_print = cfg.add_parser("print", help="print canonical merged config payload")
    c_print.add_argument("--report", choices=["text", "json"], default="json")

    c_validate = cfg.add_parser("validate", help="validate schemas, ownership, keys docs, and sync")
    c_validate.add_argument("--report", choices=["text", "json"], default="text")
    c_validate.add_argument("--emit-artifacts", action="store_true")

    c_generate = cfg.add_parser("generate", help="generate configs docs/contracts artifacts")
    c_generate.add_argument("--report", choices=["text", "json"], default="text")
    c_generate.add_argument("--check", action="store_true", help="fail if generated outputs drift")

    c_sync = cfg.add_parser("sync", help="validate or update sync outputs (SLO)")
    c_sync.add_argument("--report", choices=["text", "json"], default="text")
    c_sync.add_argument("--write", action="store_true", help="write synced output")

    c_drift = cfg.add_parser("drift", help="check generated outputs drift")
    c_drift.add_argument("--report", choices=["text", "json"], default="text")
    c_gen = cfg.add_parser("gen", help="config compiler generate deterministic configs/_generated outputs")
    c_gen.add_argument("--report", choices=["text", "json"], default="text")
    c_gen.add_argument("--check", action="store_true", help="fail if generated outputs drift")
    c_diff = cfg.add_parser("diff", help="config compiler diff generated outputs")
    c_diff.add_argument("--report", choices=["text", "json"], default="text")
    c_diff.add_argument("--fail", action="store_true", help="exit non-zero on drift")
    c_fmt = cfg.add_parser("fmt", help="canonicalize config JSON formatting")
    c_fmt.add_argument("--report", choices=["text", "json"], default="text")
    c_fmt.add_argument("--check", action="store_true", help="fail if formatting changes would be applied")
