from __future__ import annotations

import json
from datetime import date
from pathlib import Path

from atlasctl.core.schema.yaml_utils import load_yaml


_GEN_HEADER = "# GENERATED BY atlasctl configs gen. DO NOT EDIT."
_CUTOFF = date(2026, 5, 1)
_OPS_INVENTORY_SCHEMA_NONE_ALLOWLIST = {
    "ops/inventory/contracts.json",
    "ops/inventory/contracts-map.json",
    "ops/inventory/generated-committed-mirror.json",
}


def _read_json(path: Path) -> dict[str, object]:
    return json.loads(path.read_text(encoding="utf-8"))


def check_config_generated_no_human_refs(repo_root: Path) -> tuple[int, list[str]]:
    errors: list[str] = []
    for root in (repo_root / "docs", repo_root / "makefiles", repo_root / ".github/workflows", repo_root / "ops"):
        if not root.exists():
            continue
        for path in root.rglob("*"):
            if not path.is_file():
                continue
            if path.suffix not in {".md", ".mk", ".yml", ".yaml", ".json", ".txt"}:
                continue
            rel = path.relative_to(repo_root).as_posix()
            text = path.read_text(encoding="utf-8", errors="ignore")
            if "configs/_generated/" in text:
                errors.append(f"{rel}: human-facing path must not reference configs/_generated directly")
    return (0 if not errors else 1), (["config generated refs policy passed"] if not errors else errors)


def check_makefiles_no_deep_config_paths(repo_root: Path) -> tuple[int, list[str]]:
    errors: list[str] = []
    for path in (repo_root / "makefiles").rglob("*.mk"):
        text = path.read_text(encoding="utf-8", errors="ignore")
        rel = path.relative_to(repo_root).as_posix()
        for token in text.split():
            if not token.startswith("configs/"):
                continue
            depth = token.count("/")
            if depth >= 3:
                errors.append(f"{rel}: deep config path hardcoded `{token}` (atlasctl should resolve)")
    return (0 if not errors else 1), (["makefiles config path policy passed"] if not errors else errors)


def check_config_generated_headers_and_checksums(repo_root: Path) -> tuple[int, list[str]]:
    root = repo_root / "configs" / "_generated"
    if not root.exists():
        return 1, ["configs/_generated missing; run `atlasctl configs gen`"]
    checksums = root / "checksums.json"
    if not checksums.exists():
        return 1, ["configs/_generated/checksums.json missing; run `atlasctl configs gen`"]
    payload_text = checksums.read_text(encoding="utf-8")
    if not payload_text.startswith(_GEN_HEADER):
        return 1, ["configs/_generated/checksums.json missing generated header"]
    payload = json.loads(payload_text.split("\n", 1)[1])
    files = payload.get("files", [])
    errors: list[str] = []
    if not isinstance(files, list):
        return 1, ["configs/_generated/checksums.json files must be a list"]
    by_path = {str(row.get("path", "")): str(row.get("sha256", "")) for row in files if isinstance(row, dict)}
    for path in sorted(root.glob("*")):
        if not path.is_file():
            continue
        rel = path.relative_to(repo_root).as_posix()
        if rel == "configs/_generated/checksums.json":
            text = path.read_text(encoding="utf-8")
            if not text.startswith(_GEN_HEADER):
                errors.append(f"{rel}: missing generated header")
            continue
        if rel not in by_path:
            errors.append(f"missing checksum entry: {rel}")
            continue
        text = path.read_text(encoding="utf-8")
        if not text.startswith(_GEN_HEADER):
            errors.append(f"{rel}: missing generated header")
        digest = __import__("hashlib").sha256(text.encode("utf-8")).hexdigest()
        if by_path[rel] != digest:
            errors.append(f"{rel}: checksum mismatch (hand-edit or stale generation)")
    return (0 if not errors else 1), (["config generated checksum/header policy passed"] if not errors else errors)


def check_ops_inventory_schema_coverage(repo_root: Path) -> tuple[int, list[str]]:
    inv = repo_root / "ops" / "inventory"
    if not inv.exists():
        return 1, ["ops/inventory missing"]
    contract_map = _read_json(inv / "contracts-map.json")
    rows = contract_map.get("items", [])
    mapped = {str(r.get("path", "")): str(r.get("schema", "")) for r in rows if isinstance(r, dict)}
    errors: list[str] = []
    for path in sorted(inv.rglob("*")):
        if not path.is_file():
            continue
        rel = path.relative_to(repo_root).as_posix()
        if path.name == "README.md":
            continue
        if rel.startswith("ops/inventory/contracts/") and rel.endswith(".contract.fragment.json"):
            continue
        if rel not in mapped:
            errors.append(f"ops inventory file missing contracts-map entry: {rel}")
            continue
        schema = mapped[rel]
        if schema in {"none", ""} and rel not in _OPS_INVENTORY_SCHEMA_NONE_ALLOWLIST and path.suffix in {".json", ".yaml", ".yml"} and "fragments" not in rel and "/contracts/" not in rel:
            errors.append(f"ops inventory file should have schema coverage: {rel}")
    return (0 if not errors else 1), (["ops inventory schema coverage passed"] if not errors else errors)


def check_config_lock_discipline(repo_root: Path) -> tuple[int, list[str]]:
    errors: list[str] = []
    required = [
        "ops/inventory/toolchain.yaml",
        "configs/ops/pins/helm.json",
        "ops/load/queries/pinned-v1.lock",
    ]
    for rel in required:
        if not (repo_root / rel).exists():
            errors.append(f"missing config lock artifact: {rel}")
    try:
        helm = _read_json(repo_root / "configs/ops/pins/helm.json")
        if not isinstance(helm.get("helm", {}), dict):
            errors.append("configs/ops/pins/helm.json must contain `helm` object")
    except Exception as exc:
        errors.append(f"invalid helm pin lock file: {exc}")
    try:
        toolchain = load_yaml(repo_root / "ops/inventory/toolchain.yaml")
        if not isinstance(toolchain, dict):
            errors.append("ops/inventory/toolchain.yaml must be mapping")
    except Exception as exc:
        errors.append(f"invalid toolchain lock file: {exc}")
    return (0 if not errors else 1), (["config lock discipline passed"] if not errors else errors)


def check_config_migration_cutoff_legacy_locations(repo_root: Path) -> tuple[int, list[str]]:
    today = date.today()
    legacy = [p for p in (repo_root / "configs").glob("inventory/*")] if (repo_root / "configs" / "inventory").exists() else []
    if today <= _CUTOFF:
        return 0, [f"config migration cutoff guard inactive until {_CUTOFF.isoformat()}"]
    if legacy:
        return 1, [f"legacy config inventory locations forbidden after cutoff: {', '.join(p.relative_to(repo_root).as_posix() for p in legacy)}"]
    return 0, ["config migration cutoff guard passed"]
