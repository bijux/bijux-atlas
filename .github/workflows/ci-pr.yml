name: ci-pr

on:
  pull_request:
  merge_group:

permissions:
  contents: read

concurrency:
  group: ci-pr-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

env:
  RUN_ID: pr-${{ github.event.pull_request.number || github.run_id }}-${{ github.run_attempt }}
  CARGO_TERM_COLOR: always

jobs:
  route-changes:
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      docs_only: ${{ steps.filter.outputs.docs_only }}
    steps:
      - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5
      - id: filter
        uses: dorny/paths-filter@de90cc6fb38fc0963ad72b210f1f284cd68cea36
        with:
          filters: |
            docs_only:
              - 'docs/**'
              - 'mkdocs.yml'
              - '.github/**'
              - 'configs/docs/**'
              - 'configs/openapi/**'
              - 'make/docs.mk'

  minimal-root-policies:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: route-changes
    env:
      NETWORK_ACCESS: required
      ISO_ROOT: artifacts/isolates/ci-pr-minimal
      CARGO_TARGET_DIR: .cache/cargo/target/ci-pr-minimal
      CARGO_HOME: .cache/cargo/home/ci-pr-minimal
      NEXTEST_CACHE_DIR: .cache/cargo/nextest/ci-pr-minimal
      TMPDIR: artifacts/isolates/ci-pr-minimal/tmp
      TMP: artifacts/isolates/ci-pr-minimal/tmp
      TEMP: artifacts/isolates/ci-pr-minimal/tmp
    steps:
      - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5
      - uses: dtolnay/rust-toolchain@631a55b12751854ce901bb631d5902ceb48146f7
        with:
          toolchain: 1.84.1
      - name: Prepare run artifact directory
        shell: bash
        run: |
          set -euo pipefail
          rm -rf "artifacts/${RUN_ID}"
          mkdir -p "artifacts/${RUN_ID}"
      - name: Restore cargo registry cache
        id: cache-registry
        uses: actions/cache/restore@0057852bfaa89a56745cba8c7296529d2fc39830
        with:
          path: |
            .cache/cargo/home/ci-pr-minimal/registry
            .cache/cargo/home/ci-pr-minimal/git
          key: cargo-registry-pr-${{ runner.os }}-1.84.1-${{ hashFiles('Cargo.lock') }}
      - name: Restore cargo target cache
        id: cache-target
        uses: actions/cache/restore@0057852bfaa89a56745cba8c7296529d2fc39830
        with:
          path: .cache/cargo/target/ci-pr-minimal
          key: cargo-target-pr-minimal-${{ runner.os }}-1.84.1-${{ hashFiles('Cargo.lock') }}
      - name: Run minimal lane
        shell: bash
        run: |
          set -euo pipefail
          report_root="artifacts/${RUN_ID}/reports"
          logs_root="artifacts/${RUN_ID}/logs/ci-pr-minimal"
          mkdir -p "${report_root}" "${logs_root}"
          contracts_root="${report_root}/contracts"
          mkdir -p "${contracts_root}"
          cargo run -q -p bijux-dev-atlas -- contracts self-check --format json > "${contracts_root}/self-check.json" 2> "${logs_root}/contracts-self-check.stderr.log"
          make doctor > "${report_root}/doctor.json" 2> "${logs_root}/doctor.stderr.log"
          cargo run -q -p bijux-dev-atlas -- check tree-budgets --format json > "${report_root}/tree-budgets.json" 2> "${logs_root}/tree-budgets.stderr.log"
          make help > "${logs_root}/make-help.stdout.log" 2> "${logs_root}/make-help.stderr.log"
          make make-target-list > "${logs_root}/make-target-list.stdout.log" 2> "${logs_root}/make-target-list.stderr.log"
          cp make/target-list.json "${report_root}/make-target-list.json"
          if [ "${GITHUB_EVENT_NAME}" = "merge_group" ]; then
            ARTIFACT_ROOT="${report_root}" RUN_ID="${RUN_ID}" make contracts-merge > "${contracts_root}/all.json" 2> "${logs_root}/contracts-merge.stderr.log"
          else
            ARTIFACT_ROOT="${report_root}" RUN_ID="${RUN_ID}" make contracts-pr > "${contracts_root}/all.json" 2> "${logs_root}/contracts-pr.stderr.log"
          fi
          cargo run -q -p bijux-dev-atlas -- check run --domain root --tag lint --format json > "${logs_root}/lint-policy-report.stdout.log" 2> "${logs_root}/lint-policy-report.stderr.log"
          printf 'workspace_lints_file=Cargo.toml\nclippy_conf_dir=configs/rust\n' > "${report_root}/effective-clippy-policy.txt"
          cargo run -q -p bijux-dev-atlas -- check run --domain root --tag lint --format json > "${logs_root}/lint-policy-enforce.stdout.log" 2> "${logs_root}/lint-policy-enforce.stderr.log"
          CLIPPY_CONF_DIR=configs/rust cargo clippy --workspace --all-targets --all-features --locked --message-format=json -- -D warnings > "${report_root}/clippy.json" 2> "${logs_root}/lint-clippy-json.stderr.log"
          cargo tree -p bijux-atlas-store --edges normal --no-default-features --features backend-local > "${report_root}/store-deps-backend-local.txt"
          cargo tree -p bijux-atlas-store --edges normal --no-default-features --features backend-s3 > "${report_root}/store-deps-backend-s3.txt"
          local_dep_count="$(wc -l < "${report_root}/store-deps-backend-local.txt")"
          s3_dep_count="$(wc -l < "${report_root}/store-deps-backend-s3.txt")"
          [ "${s3_dep_count}" -gt "${local_dep_count}" ]
          printf '{"schema_version":1,"kind":"store_backend_dependency_surface","backend_local_count":%s,"backend_s3_count":%s}\n' "${local_dep_count}" "${s3_dep_count}" > "${report_root}/store-deps-diff.json"
          cp ops/inventory/release-build-profile.json "${report_root}/store-release-backend-profile.json"
          cargo check -q -p bijux-atlas-store --no-default-features --features backend-local > "${logs_root}/store-backend-local-check.stdout.log" 2> "${logs_root}/store-backend-local-check.stderr.log"
          cargo check -q -p bijux-atlas-store --no-default-features --features backend-s3 > "${logs_root}/store-backend-s3-check.stdout.log" 2> "${logs_root}/store-backend-s3-check.stderr.log"
          cargo test -q -p bijux-atlas-store --no-default-features --features backend-local local_backend_roundtrip_is_hermetic -- --exact > "${logs_root}/store-backend-local-test.stdout.log" 2> "${logs_root}/store-backend-local-test.stderr.log"
          cargo test -q -p bijux-atlas-store --no-default-features --features backend-s3 http_backend_reads_from_hermetic_cached_objects -- --exact > "${logs_root}/store-backend-s3-test.stdout.log" 2> "${logs_root}/store-backend-s3-test.stderr.log"
          cargo fmt --all -- --check --config-path configs/rust/rustfmt.toml > "${logs_root}/fmt-check.stdout.log" 2> "${logs_root}/fmt-check.stderr.log"
          cargo run -q -p bijux-dev-atlas -- configs verify --format json > "${report_root}/configs-verify.json" 2> "${logs_root}/configs-verify.stderr.log"
          cargo run -q -p bijux-dev-atlas -- check run --suite docs:required --include-internal --include-slow --format json > "${contracts_root}/docs-required.json" 2> "${logs_root}/docs-required.stderr.log"
          cargo run -q -p bijux-dev-atlas -- check run --suite configs:required --include-internal --include-slow --format json > "${contracts_root}/configs-required.json" 2> "${logs_root}/configs-required.stderr.log"
          cargo run -q -p bijux-dev-atlas -- check run --suite make:required --include-internal --include-slow --format json > "${contracts_root}/make-required.json" 2> "${logs_root}/make-required.stderr.log"
          cargo run -q -p bijux-dev-atlas -- check run --suite repo_required --include-internal --include-slow --allow-git --format json > "${contracts_root}/repo-required.json" 2> "${logs_root}/repo-required.stderr.log"
          cargo run -q -p bijux-dev-atlas -- check list --suite docs:required --include-internal --include-slow --format json > "${report_root}/suite-membership-docs_required.json"
          cargo run -q -p bijux-dev-atlas -- check list --suite configs:required --include-internal --include-slow --format json > "${report_root}/suite-membership-configs_required.json"
          cargo run -q -p bijux-dev-atlas -- check list --suite make:required --include-internal --include-slow --format json > "${report_root}/suite-membership-make_required.json"
          cargo run -q -p bijux-dev-atlas -- check list --suite repo_required --include-internal --include-slow --format json > "${report_root}/suite-membership-repo_required.json"
          jq -n \
            --arg run_id "${RUN_ID}" \
            --arg source "docs/_internal/contracts/repo-laws.json" \
            --arg suite "repo_required" \
            --slurpfile suite_members "${report_root}/suite-membership-repo_required.json" \
            --slurpfile run_report "${contracts_root}/repo-required.json" \
            '{schema_version:1,kind:"repo_integrity_summary",run_id:$run_id,laws_source:$source,required_suite:$suite,checks_total:($suite_members[0].checks|length),counts:$run_report[0].counts,p0_checks:$suite_members[0].checks|map(.id)}' \
            > "${report_root}/repo-integrity-summary.json"
          base_ref="${GITHUB_BASE_REF:-main}"
          base_sha="$(git merge-base HEAD "origin/${base_ref}" || true)"
          if [ -z "${base_sha}" ]; then
            base_sha="$(git rev-parse HEAD~1)"
          fi
          git diff --name-status "${base_sha}...HEAD" > "${report_root}/repo-surface-diff.txt"
          export REPORT_ROOT="${report_root}"
          export BASE_SHA="${base_sha}"
          python - <<'PY'
          import json, pathlib, os
          root = pathlib.Path(os.environ["REPORT_ROOT"])
          lines = [ln.strip() for ln in (root / "repo-surface-diff.txt").read_text().splitlines() if ln.strip()]
          rows = []
          root_rows = []
          for line in lines:
            parts = line.split("\t")
            status = parts[0]
            paths = [p for p in parts[1:] if p]
            rows.append({"status": status, "paths": paths})
            for p in paths:
              if "/" not in p:
                root_rows.append({"status": status, "path": p})
          (root / "repo-surface-diff.json").write_text(json.dumps({
              "schema_version": 1,
              "kind": "repo_surface_diff",
              "base": os.environ.get("BASE_SHA", ""),
              "head": os.environ.get("GITHUB_SHA", ""),
              "changes": rows,
          }, indent=2) + "\n")
          (root / "root-change-report.json").write_text(json.dumps({
              "schema_version": 1,
              "kind": "root_change_report",
              "base": os.environ.get("BASE_SHA", ""),
              "head": os.environ.get("GITHUB_SHA", ""),
              "root_changes": root_rows,
          }, indent=2) + "\n")
          PY
          root_changed_count="$(jq -r '.root_changes | length' "${report_root}/root-change-report.json")"
          if [ "${root_changed_count}" -gt 0 ]; then
            cargo run -q -p bijux-dev-atlas -- contracts root --mode static --format json --artifacts-root "${contracts_root}" > "${contracts_root}/root-strict.json" 2> "${logs_root}/root-strict.stderr.log"
          fi
          semantic_target_count="$(jq -r '[.root_changes[] | select(.path == \"README.md\" or .path == \"CONTRACT.md\")] | length' "${report_root}/root-change-report.json")"
          if [ "${semantic_target_count}" -gt 0 ] && [ "${GITHUB_EVENT_NAME}" = "pull_request" ]; then
            pr_body="$(jq -r '.pull_request.body // \"\"' \"${GITHUB_EVENT_PATH}\")"
            has_label="$(jq -r '[.pull_request.labels[]?.name | select(. == \"root-semantic-diff-approved\")] | length' \"${GITHUB_EVENT_PATH}\")"
            if ! printf '%s' "${pr_body}" | grep -q '\\[root-semantic-diff\\]' && [ "${has_label}" -eq 0 ]; then
              echo \"missing required root semantic diff marker: add [root-semantic-diff] to PR description or label root-semantic-diff-approved\" >&2
              exit 1
            fi
          fi
          cargo run -q -p bijux-dev-atlas -- governance validate --format json > "${report_root}/governance-validate.json" 2> "${logs_root}/governance-validate.stderr.log"
          coverage_current="$(jq -r '.coverage_percent' artifacts/governance/governance-coverage.json)"
          coverage_baseline="$(jq -r '.coverage_percent_min' governance/coverage-baseline.json)"
          delta="$(awk -v c="${coverage_current}" -v b="${coverage_baseline}" 'BEGIN { printf \"%.2f\", c-b }')"
          status="ok"
          if awk -v c="${coverage_current}" -v b="${coverage_baseline}" 'BEGIN { exit !(c < b) }'; then
            status="failed"
            echo "governance coverage dropped below baseline: current=${coverage_current} baseline=${coverage_baseline}" >&2
            exit 1
          fi
          jq -n \
            --arg status "${status}" \
            --argjson current "${coverage_current}" \
            --argjson baseline "${coverage_baseline}" \
            --arg delta "${delta}" \
            '{
              schema_version: 1,
              kind: "governance_coverage_diff",
              status: $status,
              baseline_percent: $baseline,
              current_percent: $current,
              delta_percent: ($delta | tonumber)
            }' > "${report_root}/governance-coverage-diff.json"
          changed_list="$(git diff --name-only "${base_sha}...HEAD")"
          changed_file() {
            printf '%s\n' "${changed_list}" | grep -qx "$1"
          }
          python - <<'PY'
          import json, os, pathlib, subprocess
          base = os.environ.get("BASE_SHA", "").strip()
          head = os.environ.get("GITHUB_SHA", "").strip()
          report_root = pathlib.Path(os.environ["REPORT_ROOT"])
          changed = []
          if base:
              out = subprocess.check_output(["git", "diff", "--name-only", f"{base}...HEAD"], text=True)
              changed = [line.strip() for line in out.splitlines() if line.strip()]
          def subset(prefixes):
              return sorted([p for p in changed if any(p.startswith(px) for px in prefixes)])
          reports = {
              "schema_version": 1,
              "kind": "semantic_drift_reports",
              "base": base,
              "head": head,
              "configs": {"changed": subset(["configs/", "crates/bijux-atlas-server/docs/generated/effective-config.snapshot.json", "docs/reference/contracts/schemas/CONFIG_KEYS.json"])},
              "docs": {"changed": subset(["docs/"])},
              "make": {"changed": subset(["make/", "configs/ops/make-target-registry.json", "make/target-list.json"])},
              "docker": {"changed": subset(["docker/", "docker/images.manifest.json", "docker/policy.json"])},
              "ops_readiness": {"changed": subset(["ops/observe/readiness.json", "ops/report/evidence-levels.json", "ops/report/generated/readiness-score.json", "ops/report/generated/release-evidence-bundle.json"])},
          }
          for domain in ["configs", "docs", "make", "docker", "ops_readiness"]:
              reports[domain]["status"] = "changed" if reports[domain]["changed"] else "unchanged"
              reports[domain]["count"] = len(reports[domain]["changed"])
              path = report_root / f"semantic-drift-{domain}.json"
              path.write_text(json.dumps({
                  "schema_version": 1,
                  "kind": f"semantic_drift_{domain}",
                  "base": base,
                  "head": head,
                  "status": reports[domain]["status"],
                  "changed": reports[domain]["changed"],
                  "count": reports[domain]["count"],
              }, indent=2) + "\n")
          (report_root / "semantic-drift-summary.json").write_text(json.dumps(reports, indent=2) + "\n")
          PY
          pr_body="$(jq -r '.pull_request.body // \"\"' "${GITHUB_EVENT_PATH}")"
          if changed_file "configs/openapi/v1/openapi.snapshot.json"; then
            if ! printf '%s' "${pr_body}" | grep -Eq '\[api-compat:(non-breaking|breaking)\]'; then
              echo "OpenAPI snapshot changed but compatibility marker is missing; add [api-compat:non-breaking] or [api-compat:breaking] in PR body" >&2
              exit 1
            fi
          fi
          endpoints_changed=0
          openapi_changed=0
          if changed_file "docs/reference/contracts/schemas/ENDPOINTS.json"; then endpoints_changed=1; fi
          if changed_file "configs/openapi/v1/openapi.snapshot.json"; then openapi_changed=1; fi
          if [ "${endpoints_changed}" -ne "${openapi_changed}" ]; then
            echo "ENDPOINTS registry diff must match OpenAPI snapshot diff" >&2
            exit 1
          fi
          error_registry_changed=0
          error_runtime_changed=0
          if changed_file "docs/reference/contracts/schemas/ERROR_CODES.json"; then error_registry_changed=1; fi
          if changed_file "crates/bijux-atlas-api/src/generated/error_codes.rs" || changed_file "crates/bijux-atlas-core/src/generated/error_codes.rs" || changed_file "ops/inventory/meta/error-registry.json"; then error_runtime_changed=1; fi
          if [ "${error_registry_changed}" -ne "${error_runtime_changed}" ]; then
            echo "ERROR_CODES registry diff must match runtime error map diff" >&2
            exit 1
          fi
          config_registry_changed=0
          config_runtime_changed=0
          if changed_file "docs/reference/contracts/schemas/CONFIG_KEYS.json"; then config_registry_changed=1; fi
          if changed_file "crates/bijux-atlas-server/docs/generated/effective-config.snapshot.json" || changed_file "crates/bijux-atlas-server/docs/generated/runtime-startup-config.md" || changed_file "crates/bijux-atlas-server/docs/generated/runtime-startup-config.schema.json"; then config_runtime_changed=1; fi
          if [ "${config_registry_changed}" -ne "${config_runtime_changed}" ]; then
            echo "CONFIG_KEYS registry diff must match runtime effective config artifact diff" >&2
            exit 1
          fi
          # Generated reference drift mapping from SSOT surfaces.
          ssot_docs_pairs=0
          if changed_file "ops/inventory/root-surface.json"; then
            ssot_docs_pairs=1
            changed_file "docs/reference/repo-map.md" || { echo "root surface SSOT changed but docs/reference/repo-map.md was not updated" >&2; exit 1; }
          fi
          if changed_file "configs/ops/make-target-registry.json"; then
            ssot_docs_pairs=1
            changed_file "docs/_internal/generated/make-targets.md" || { echo "make target registry changed but docs/_internal/generated/make-targets.md was not updated" >&2; exit 1; }
          fi
          if changed_file "docs/_internal/registry/registry.json"; then
            ssot_docs_pairs=1
            changed_file "docs/_internal/governance/metadata/front-matter.index.json" || { echo "docs registry changed but front-matter index was not updated" >&2; exit 1; }
          fi
          if [ "${ssot_docs_pairs}" -eq 1 ]; then
            jq -n '{schema_version:1,kind:"docs_ssot_reference_drift_gate",status:"ok"}' > "${report_root}/docs-ssot-reference-drift.json"
          else
            jq -n '{schema_version:1,kind:"docs_ssot_reference_drift_gate",status:"not_applicable"}' > "${report_root}/docs-ssot-reference-drift.json"
          fi
          # Golden workflow docs must include executable command and expected output guidance.
          rg -n "^bijux dev atlas demo quickstart --format json$" docs/start-here.md > /dev/null
          rg -n "Expected output:" docs/start-here.md docs/control-plane/lane-matrix.md > /dev/null
          jq -n '{schema_version:1,kind:"docs_golden_workflow_verification",status:"ok"}' > "${report_root}/docs-golden-workflow-verification.json"
          # Docs external link allowlist requires expiry and owner for each entry.
          python - <<'PY'
          import json, pathlib
          allow = json.loads(pathlib.Path("configs/docs/external-link-allowlist.json").read_text())
          entries = allow.get("entries", [])
          errors = []
          for i, row in enumerate(entries):
              if not row.get("pattern"):
                  errors.append(f"entries[{i}] missing pattern")
              if not row.get("owner"):
                  errors.append(f"entries[{i}] missing owner")
              if not row.get("expires_on"):
                  errors.append(f"entries[{i}] missing expires_on")
          if errors:
              raise SystemExit("docs external-link allowlist contract failed: " + "; ".join(errors))
          pathlib.Path(__import__("os").environ["REPORT_ROOT"], "docs-link-allowlist-policy.json").write_text(json.dumps({
              "schema_version": 1,
              "kind": "docs_link_allowlist_policy",
              "status": "ok",
              "entries": len(entries)
          }, indent=2) + "\n")
          PY
          # Alert/runbook and SLO/load/e2e mapping contracts.
          python - <<'PY'
          import json, pathlib, re
          root = pathlib.Path(".")
          report_root = pathlib.Path(__import__("os").environ["REPORT_ROOT"])
          alerts = json.loads((root / "ops/observe/contracts/alerts-contract.json").read_text())["alert_specs"]
          alert_catalog = {r["id"] for r in json.loads((root / "ops/observe/alert-catalog.json").read_text())["alerts"]}
          runbook_dir = root / "docs/operations/runbooks"
          runbook_ids = {p.stem for p in runbook_dir.glob("*.md")}
          missing_alerts = sorted([k for k in alerts.keys() if k not in alert_catalog])
          missing_runbooks = sorted([k for k,v in alerts.items() if v.get("runbook_id","") not in runbook_ids])
          if missing_alerts or missing_runbooks:
              raise SystemExit(f"alerts/runbooks mapping failed missing_alerts={missing_alerts} missing_runbooks={missing_runbooks}")
          slo = json.loads((root / "configs/slo/slo.json").read_text())
          slis = {r["id"] for r in slo["slis"]}
          slos = slo["slos"]
          bad_slos = [row["id"] for row in slos if row.get("sli") not in slis]
          if bad_slos:
              raise SystemExit("slo mapping failed: unknown sli in " + ", ".join(sorted(bad_slos)))
          dashboards = list((root / "ops/observe/dashboards").glob("*.json"))
          if not dashboards:
              raise SystemExit("slo mapping failed: no dashboards found under ops/observe/dashboards")
          load_suites = json.loads((root / "ops/load/generated/suites.manifest.json").read_text())["suites"]
          capacity_doc = root / "docs/operations/load/suites.md"
          if not capacity_doc.exists():
              raise SystemExit("load suite mapping failed: docs/operations/load/suites.md missing")
          no_purpose = [row["name"] for row in load_suites if not row.get("purpose")]
          if no_purpose:
              raise SystemExit("load suites missing purpose: " + ", ".join(sorted(no_purpose)))
          e2e_scenarios = json.loads((root / "ops/e2e/scenarios/scenarios.json").read_text())["scenarios"]
          triage_doc = root / "docs/operations/runbooks/load-failure-triage.md"
          if not triage_doc.exists():
              raise SystemExit("e2e mapping failed: load-failure-triage runbook missing")
          bad_e2e = [row["id"] for row in e2e_scenarios if not row.get("entrypoint")]
          if bad_e2e:
              raise SystemExit("e2e scenarios missing entrypoint: " + ", ".join(sorted(bad_e2e)))
          (report_root / "ops-alert-runbook-slo-load-e2e-map.json").write_text(json.dumps({
              "schema_version": 1,
              "kind": "ops_alert_runbook_slo_load_e2e_map",
              "status": "ok",
              "alert_count": len(alerts),
              "runbook_count": len(runbook_ids),
              "slo_count": len(slos),
              "sli_count": len(slis),
              "load_suite_count": len(load_suites),
              "e2e_scenario_count": len(e2e_scenarios)
          }, indent=2) + "\n")
          PY
          # Registry contract groups must not be empty (domain and suite).
          python - <<'PY'
          import pathlib, tomllib, json
          report_root = pathlib.Path(__import__("os").environ["REPORT_ROOT"])
          reg = tomllib.loads(pathlib.Path("ops/inventory/registry.toml").read_text())
          checks = reg.get("checks", [])
          suites = reg.get("suites", [])
          domain_counts = {}
          for row in checks:
              d = row.get("domain","").strip()
              domain_counts[d] = domain_counts.get(d,0)+1
          empty_domains = sorted([d for d,c in domain_counts.items() if d and c == 0])
          empty_suite_refs = sorted([s.get("id","") for s in suites if not s.get("checks")])
          if empty_domains or empty_suite_refs:
              raise SystemExit(f"registry empty groups found domains={empty_domains} suites={empty_suite_refs}")
          (report_root / "registry-group-coverage.json").write_text(json.dumps({
              "schema_version": 1,
              "kind": "registry_group_coverage",
              "status": "ok",
              "domains": domain_counts,
              "suite_count": len(suites)
          }, indent=2) + "\n")
          PY
          # Make surface must derive from one source.
          jq -r '.public_targets[]' make/target-list.json | sort > "${report_root}/make-public-targets.txt"
          jq -r '.targets[] | select(.visibility=="public") | .name' configs/ops/make-target-registry.json | sort > "${report_root}/make-registry-public-targets.txt"
          if ! diff -u "${report_root}/make-registry-public-targets.txt" "${report_root}/make-public-targets.txt" > "${report_root}/make-surface-drift.diff"; then
            echo "public make target surface drift: make/target-list.json does not match configs/ops/make-target-registry.json" >&2
            exit 1
          fi
          jq -n --rawfile registry "${report_root}/make-registry-public-targets.txt" --rawfile surface "${report_root}/make-public-targets.txt" '{
            schema_version: 1,
            kind: "make_surface_drift",
            status: "ok",
            registry_public_targets: ($registry | split("\n") | map(select(length>0))),
            surfaced_public_targets: ($surface | split("\n") | map(select(length>0)))
          }' > "${report_root}/make-surface-drift.json"
          # Deterministic check run order report.
          python - <<'PY'
          import json, pathlib
          report_root = pathlib.Path(__import__("os").environ["REPORT_ROOT"])
          for suite in ["docs_required", "configs_required", "make_required", "repo_required", "ci_fast"]:
              path = report_root / f"suite-membership-{suite}.json"
              if not path.exists():
                  continue
              payload = json.loads(path.read_text())
              ids = [row["id"] for row in payload.get("checks", [])]
              if ids != sorted(ids):
                  raise SystemExit(f"suite membership order is not deterministic for {suite}")
          (report_root / "deterministic-check-order.json").write_text(json.dumps({
              "schema_version": 1,
              "kind": "deterministic_check_order",
              "status": "ok",
          }, indent=2) + "\n")
          PY
          # Check metadata report for purpose/failure/fix and stable machine error code.
          python - <<'PY'
          import json, pathlib, re, tomllib
          repo = pathlib.Path(".")
          report_root = pathlib.Path(__import__("os").environ["REPORT_ROOT"])
          reg = tomllib.loads((repo / "ops/inventory/registry.toml").read_text())
          checks = []
          for row in reg.get("checks", []):
              cid = row["id"].strip()
              docs = row["docs"].strip()
              title = row["title"].strip()
              if not cid or not title or not docs:
                  raise SystemExit(f"registry check metadata missing required fields for {cid or '<unknown>'}")
              checks.append({
                  "id": cid,
                  "purpose": title,
                  "failure_mode": f"{cid} emits violation records when invariant is broken",
                  "fix_hint": f"follow {docs}",
                  "error_code": cid.replace("-", "_").upper(),
              })
          checks.sort(key=lambda r: r["id"])
          (report_root / "check-metadata-catalog.json").write_text(json.dumps({
              "schema_version": 1,
              "kind": "check_metadata_catalog",
              "checks": checks,
          }, indent=2) + "\n")
          PY
          # Evidence artifacts must be schema-versioned where applicable.
          python - <<'PY'
          import json, pathlib
          report_root = pathlib.Path(__import__("os").environ["REPORT_ROOT"])
          missing = []
          for path in sorted(report_root.glob("*.json")):
              try:
                  payload = json.loads(path.read_text())
              except Exception:
                  continue
              if isinstance(payload, dict):
                  if "schema_version" not in payload:
                      missing.append(path.name)
          if missing:
              raise SystemExit("report JSON missing schema_version: " + ", ".join(missing))
          (report_root / "artifact-schema-version-check.json").write_text(json.dumps({
              "schema_version": 1,
              "kind": "artifact_schema_version_check",
              "status": "ok",
          }, indent=2) + "\n")
          PY
          # Artifact naming and size budget.
          python - <<'PY'
          import json, os, pathlib, re
          report_root = pathlib.Path(os.environ["REPORT_ROOT"])
          bad_names = []
          bad_sizes = []
          for path in report_root.rglob("*"):
              if not path.is_file():
                  continue
              rel = path.relative_to(report_root).as_posix()
              if not re.fullmatch(r"[a-z0-9][a-z0-9._/-]*", rel):
                  bad_names.append(rel)
              if path.stat().st_size > 5 * 1024 * 1024:
                  bad_sizes.append({"path": rel, "size": path.stat().st_size})
          if bad_names:
              raise SystemExit("artifact naming policy violation: " + ", ".join(bad_names))
          if bad_sizes:
              raise SystemExit("artifact size budget exceeded: " + ", ".join(f"{row['path']}={row['size']}" for row in bad_sizes))
          (report_root / "artifact-naming-and-size.json").write_text(json.dumps({
              "schema_version": 1,
              "kind": "artifact_naming_and_size",
              "status": "ok",
          }, indent=2) + "\n")
          PY
          cargo run -q -p bijux-dev-atlas -- contracts runtime --mode static --format json --artifacts-root "${contracts_root}" > "${contracts_root}/runtime-static.json" 2> "${logs_root}/runtime-contracts-static.stderr.log"
          cargo run -q -p bijux-dev-atlas -- contracts control-plane --mode static --format json --artifacts-root "${contracts_root}" > "${contracts_root}/control-plane-static.json" 2> "${logs_root}/control-plane-contracts-static.stderr.log"
          cargo run -q -p bijux-dev-atlas -- contracts configs --mode static --format json --artifacts-root "${contracts_root}" > "${contracts_root}/configs-static.json" 2> "${logs_root}/configs-contracts-static.stderr.log"
          cargo test --workspace --locked > "${logs_root}/workspace-test.stdout.log" 2> "${logs_root}/workspace-test.stderr.log"
          cargo test -q -p bijux-atlas-api openapi_snapshot_is_deterministic_and_matches_committed_contract -- --exact > "${logs_root}/openapi-snapshot.stdout.log" 2> "${logs_root}/openapi-snapshot.stderr.log"
          cargo test -q -p bijux-atlas-api openapi_hash_matches_pinned_contract -- --exact > "${logs_root}/openapi-hash.stdout.log" 2> "${logs_root}/openapi-hash.stderr.log"
          cargo test -q -p bijux-atlas-server runtime_startup_config_contract_artifacts_match_generated > "${logs_root}/runtime-startup-config-contract.stdout.log" 2> "${logs_root}/runtime-startup-config-contract.stderr.log"
          cargo run -q -p bijux-dev-atlas -- contracts make --mode static --format text --artifacts-root "${contracts_root}" > "${contracts_root}/make-static.txt" 2> "${logs_root}/make-contracts-static.stderr.log"
          cargo run -q -p bijux-dev-atlas -- contracts docker --mode static --format json --artifacts-root "${contracts_root}" > "${contracts_root}/docker-static.json" 2> "${logs_root}/docker-contracts-static.stderr.log"
          cargo run -q -p bijux-dev-atlas -- contracts ops --mode static --format json --artifacts-root "${contracts_root}" > "${contracts_root}/ops-static.json" 2> "${logs_root}/ops-contracts-static.stderr.log"
          cargo test -q -p bijux-dev-atlas --test ops_contract_registry_snapshot -- --nocapture > "${logs_root}/ops-contract-snapshot.stdout.log" 2> "${logs_root}/ops-contract-snapshot.stderr.log"
          for file in \
            ops/inventory/contract-gate-map.json \
            ops/inventory/surfaces.json \
            ops/schema/generated/schema-index.json \
            ops/_generated.example/contracts-registry-snapshot.json; do
            jq -S . "${file}" > "${file}.canon"
            mv "${file}.canon" "${file}"
          done
          git diff --exit-code -- \
            ops/inventory/contract-gate-map.json \
            ops/inventory/surfaces.json \
            ops/schema/generated/schema-index.json \
            ops/_generated.example/contracts-registry-snapshot.json > "${logs_root}/contracts-canonical-diff.log"
          jq -r '.tests[] | select(.status=="FAIL" or .status=="ERROR") | "\(.contract_id) \(.test_id) \(.status) \((.violations | map(.message) | join("; ")))"' \
            "${contracts_root}/ops-static.json" | while IFS= read -r row; do
            [ -z "${row}" ] && continue
            contract_id="$(echo "${row}" | awk '{print $1}')"
            test_id="$(echo "${row}" | awk '{print $2}')"
            message="$(echo "${row}" | cut -d' ' -f4-)"
            echo "::error title=OPS Contract ${contract_id}/${test_id}::${message}"
          done
          jq -r '.tests[] | select(.status=="FAIL" or .status=="ERROR") | "\(.contract_id) \(.test_id) \((.violations | map(.message) | join("; ")))"' \
            "${contracts_root}/ops-static.json" > "${contracts_root}/ops-failures.txt"
          jq -r '
            . as $r
            | $r.tests
            | group_by((.contract_id | split("-")[1]))[]
            | {
                pillar: (.[0].contract_id | split("-")[1]),
                contracts: (map(.contract_id) | unique | length),
                tests: length,
                pass: (map(select(.status=="PASS")) | length),
                fail: (map(select(.status=="FAIL")) | length),
                skip: (map(select(.status=="SKIP")) | length),
                error: (map(select(.status=="ERROR")) | length),
                pass_rate: ((map(select(.status=="PASS")) | length) / length)
              }
          ' "${contracts_root}/ops-static.json" > "${contracts_root}/ops-coverage-by-pillar.json"
          jq -n --argfile ops "${contracts_root}/ops-static.json" --argfile docker "${contracts_root}/docker-static.json" --argfile pillar "${contracts_root}/ops-coverage-by-pillar.json" '
            {
              schema_version: 1,
              kind: "contracts_coverage_v1",
              ops_summary: $ops.summary,
              docker_summary: $docker.summary,
              pillar_coverage: $pillar
            }
          ' > "${contracts_root}/coverage-summary.json"
          # Coverage heatmap + missing proof + proof debt ledger + proof-level guardrails.
          python - <<'PY'
          import json, pathlib, tomllib
          report_root = pathlib.Path(__import__("os").environ["REPORT_ROOT"])
          contracts_root = report_root / "contracts"
          reg = tomllib.loads(pathlib.Path("ops/inventory/registry.toml").read_text())
          checks = reg.get("checks", [])
          def classify(title: str) -> str:
              t = title.lower()
              shape_tokens = ["exists", "present", "path", "naming", "sorted", "format", "size", "budget", "allowlist"]
              return "shape" if any(tok in t for tok in shape_tokens) else "proof"
          rows = []
          by_domain = {}
          for row in checks:
              dom = row["domain"]
              kind = classify(row["title"])
              rows.append({"id": row["id"], "domain": dom, "title": row["title"], "classification": kind})
              by_domain.setdefault(dom, {"shape": 0, "proof": 0})
              by_domain[dom][kind] += 1
          heatmap = {
              "schema_version": 1,
              "kind": "contract_coverage_heatmap",
              "domains": by_domain
          }
          (report_root / "contract-coverage-heatmap.json").write_text(json.dumps(heatmap, indent=2) + "\n")
          missing_proof = [r for r in rows if r["classification"] == "shape"]
          (report_root / "missing-proof-report.json").write_text(json.dumps({
              "schema_version": 1,
              "kind": "missing_proof_report",
              "shape_only_checks": missing_proof
          }, indent=2) + "\n")
          (report_root / "proof-debt-ledger.json").write_text(json.dumps({
              "schema_version": 1,
              "kind": "proof_debt_ledger",
              "count": len(missing_proof),
              "items": missing_proof
          }, indent=2) + "\n")
          policy = json.loads(pathlib.Path("ops/policy/proof-level-checks.json").read_text())
          required = set(policy.get("proof_level_check_ids", []))
          known = {r["id"] for r in rows}
          missing = sorted(required - known)
          if len(required) < 10 or missing:
              raise SystemExit(f"proof-level checks policy invalid len={len(required)} missing={missing}")
          (report_root / "proof-level-policy-check.json").write_text(json.dumps({
              "schema_version": 1,
              "kind": "proof_level_policy_check",
              "status": "ok",
              "required_count": len(required)
          }, indent=2) + "\n")
          PY
          # Flake detector: rerun root static contracts and compare normalized result.
          cargo run -q -p bijux-dev-atlas -- contracts root --mode static --format json --artifacts-root "${contracts_root}" > "${contracts_root}/root-static-first.json" 2> "${logs_root}/root-static-first.stderr.log"
          cargo run -q -p bijux-dev-atlas -- contracts root --mode static --format json --artifacts-root "${contracts_root}" > "${contracts_root}/root-static-rerun.json" 2> "${logs_root}/root-static-rerun.stderr.log"
          jq 'del(.summary.duration_ms) | .tests |= map(del(.duration_ms))' "${contracts_root}/root-static-first.json" > "${contracts_root}/root-static-first.normalized.json"
          jq 'del(.summary.duration_ms) | .tests |= map(del(.duration_ms))' "${contracts_root}/root-static-rerun.json" > "${contracts_root}/root-static-rerun.normalized.json"
          if ! diff -u "${contracts_root}/root-static-first.normalized.json" "${contracts_root}/root-static-rerun.normalized.json" > "${report_root}/contract-flake-detector.diff"; then
            echo "contract flake detector failed: root static rerun produced different normalized results" >&2
            exit 1
          fi
          jq -n '{schema_version:1,kind:"contract_flake_detector",status:"ok"}' > "${report_root}/contract-flake-detector.json"
          # Contract performance budgets with expiring waivers.
          python - <<'PY'
          import json, pathlib, datetime
          root = pathlib.Path(".")
          report_root = pathlib.Path(__import__("os").environ["REPORT_ROOT"])
          contracts_root = report_root / "contracts"
          policy = json.loads((root / "ops/policy/contracts-performance-budget.json").read_text())
          budgets = policy.get("budgets_ms", {})
          waivers = policy.get("waivers", [])
          today = datetime.date.fromisoformat("2026-03-01")
          active_waivers = {}
          for row in waivers:
              exp = datetime.date.fromisoformat(row["expires_on"])
              if exp < today:
                  raise SystemExit(f"expired contract performance waiver: {row}")
              active_waivers[row["domain"]] = row
          files = {
              "runtime": contracts_root / "runtime-static.json",
              "control_plane": contracts_root / "control-plane-static.json",
              "configs": contracts_root / "configs-static.json",
              "docker": contracts_root / "docker-static.json",
              "ops": contracts_root / "ops-static.json",
              "root": contracts_root / "root-static-first.json",
          }
          measured = {}
          over = []
          for domain, path in files.items():
              payload = json.loads(path.read_text())
              dur = int(payload.get("summary", {}).get("duration_ms", 0))
              measured[domain] = dur
              budget = int(budgets.get(domain, 0))
              if budget and dur > budget and domain not in active_waivers:
                  over.append((domain, dur, budget))
          if over:
              raise SystemExit("contract performance budget exceeded: " + "; ".join(f"{d} {m}>{b}" for d,m,b in over))
          (report_root / "contract-performance-budget.json").write_text(json.dumps({
              "schema_version": 1,
              "kind": "contract_performance_budget",
              "status": "ok",
              "measured_ms": measured,
              "budgets_ms": budgets,
              "active_waivers": active_waivers
          }, indent=2) + "\n")
          PY
          make ops-fast > "${report_root}/ci-fast.json" 2> "${logs_root}/ci-fast.stderr.log"
          cargo run -q -p bijux-dev-atlas -- check list --suite ci_fast --include-internal --include-slow --format json > "${report_root}/suite-membership-ci_fast.json"
          cargo run -q -p bijux-dev-atlas -- demo quickstart --format json > "${report_root}/demo-quickstart.json"
          python - <<'PY'
          import json, os, pathlib
          run_id=os.environ["RUN_ID"]
          p=pathlib.Path(f"artifacts/{run_id}/reports/demo-quickstart.json")
          payload=json.loads(p.read_text())
          assert payload["schema_version"]==1
          assert payload["steps_budget"] <= 5
          assert payload["duration_budget_minutes"] <= 3
          assert len(payload["steps"]) <= payload["steps_budget"]
          PY
          # Docs entrypoints/nav/metadata/directory-budget contracts.
          python - <<'PY'
          import json, pathlib, yaml, os
          root = pathlib.Path(".")
          report_root = pathlib.Path(os.environ["REPORT_ROOT"])
          mkdocs = yaml.safe_load((root / "mkdocs.yml").read_text())
          nav = mkdocs.get("nav", [])
          top_names = []
          def top_label(item):
              if isinstance(item, dict):
                  return next(iter(item.keys()))
              return str(item)
          top_names = [top_label(x) for x in nav]
          if top_names[:2] != ["Home", "Start Here"]:
              raise SystemExit(f"docs entrypoint contract failed: first two nav items must be Home/Start Here, got {top_names[:2]}")
          nav_text = (root / "mkdocs.yml").read_text()
          if "_internal/" in nav_text:
              raise SystemExit("published nav must not include docs/_internal pages")
          registry = json.loads((root / "docs/_internal/registry/registry.json").read_text())
          missing_meta = []
          for row in registry.get("documents", []):
              path = row.get("path", "")
              if not path.startswith("docs/") or path.startswith("docs/_internal/"):
                  continue
              required = ["audience", "title", "stability", "owner", "last_reviewed"]
              for key in required:
                  if not row.get(key):
                      missing_meta.append(f"{path}:{key}")
          if missing_meta:
              raise SystemExit("published docs metadata missing required fields: " + ", ".join(missing_meta[:20]))
          limits = json.loads((root / "docs/_internal/governance/metadata/directory-budget-exceptions.json").read_text())
          max_files = int(limits["max_files"])
          max_subdirs = int(limits["max_subdirs"])
          exceptions = {row["path"]: row for row in limits.get("exceptions", [])}
          violations = []
          for d in sorted((root / "docs").rglob("*")):
              if not d.is_dir():
                  continue
              rel = d.as_posix()
              if "/_internal/generated" in rel:
                  continue
              files = [p for p in d.iterdir() if p.is_file() and not p.name.startswith(".")]
              dirs = [p for p in d.iterdir() if p.is_dir() and not p.name.startswith(".")]
              if len(files) <= max_files and len(dirs) <= max_subdirs:
                  continue
              if rel in exceptions:
                  continue
              violations.append({"path": rel, "files": len(files), "subdirs": len(dirs)})
          if violations:
              raise SystemExit("docs directory budget exceeded without exception proof: " + json.dumps(violations[:20]))
          (report_root / "docs-entrypoint-and-budget-contracts.json").write_text(json.dumps({
              "schema_version": 1,
              "kind": "docs_entrypoint_and_budget_contracts",
              "status": "ok",
              "top_nav": top_names,
              "published_docs_count": len([r for r in registry.get("documents", []) if str(r.get("path","")).startswith("docs/") and not str(r.get("path","")).startswith("docs/_internal/")]),
              "directory_exception_count": len(exceptions)
          }, indent=2) + "\n")
          PY
          python - <<'PY'
          import json, os, pathlib, re
          root = pathlib.Path(".")
          report_root = pathlib.Path(os.environ["REPORT_ROOT"])
          docs_root = root / "docs"
          spine_pages = [
              "docs/index.md",
              "docs/start-here.md",
              "docs/product/what-is-bijux-atlas.md",
              "docs/architecture/index.md",
              "docs/operations/index.md",
              "docs/control-plane/index.md",
              "docs/api/index.md",
              "docs/development/index.md",
              "docs/reference/index.md",
          ]
          required_glossary_terms = [
              "Atlas",
              "Contract",
              "Reader spine",
              "Runbook",
              "Reference",
              "Control plane",
              "Reproducibility",
              "Release",
              "Dataset",
          ]
          # 1) Every published docs directory must expose index.md.
          index_violations = []
          for d in sorted(docs_root.rglob("*")):
              if not d.is_dir():
                  continue
              rel = d.as_posix()
              if rel.startswith("docs/_internal/") or rel.startswith("docs/_assets/") or rel.startswith("docs/_drafts/"):
                  continue
              entries = [p for p in d.iterdir() if not p.name.startswith(".")]
              if not entries:
                  continue
              if not (d / "index.md").exists():
                  index_violations.append(rel)
          if index_violations:
              raise SystemExit("docs directory index contract failed; missing index.md in: " + ", ".join(index_violations[:20]))
          # 2) Home page keeps exactly three curated reader tracks.
          home = (root / "docs/index.md").read_text()
          m = re.search(r"## Choose your path\\n\\n(?P<body>.*?)(\\n## |\\Z)", home, flags=re.S)
          if not m:
              raise SystemExit("docs/index.md must include a `## Choose your path` section")
          track_lines = [ln.strip()[2:].strip() for ln in m.group("body").splitlines() if ln.strip().startswith("- ")]
          expected_tracks = [
              "API user: begin with API",
              "Operator: begin with Operations",
              "Contributor: begin with Development",
          ]
          if track_lines != expected_tracks:
              raise SystemExit(f"docs/index.md choose-your-path contract failed: expected {expected_tracks}, got {track_lines}")
          # 3) Architecture / Operations / Control-plane must reference the product narrative spine.
          expected_spine_link = "../product/what-is-bijux-atlas.md"
          spine_link_violations = []
          for rel in ("docs/architecture/index.md", "docs/operations/index.md", "docs/control-plane/index.md"):
              txt = (root / rel).read_text()
              if expected_spine_link not in txt:
                  spine_link_violations.append(rel)
          if spine_link_violations:
              raise SystemExit("spine reference contract failed; missing product narrative link in: " + ", ".join(spine_link_violations))
          # 4) Glossary must define required terms and spine pages must link glossary.
          glossary_text = (root / "docs/glossary.md").read_text()
          defined = set(re.findall(r"^- `([^`]+)`:", glossary_text, flags=re.M))
          missing_terms = [t for t in required_glossary_terms if t not in defined]
          if missing_terms:
              raise SystemExit("glossary contract failed; missing required terms: " + ", ".join(missing_terms))
          glossary_link_violations = []
          for rel in spine_pages:
              txt = (root / rel).read_text()
              if "glossary.md" not in txt:
                  glossary_link_violations.append(rel)
          if glossary_link_violations:
              raise SystemExit("spine glossary-link contract failed: " + ", ".join(glossary_link_violations))
          # 5) Orphan report with where-to-link hints.
          all_docs = []
          for path in docs_root.rglob("*.md"):
              rel = path.as_posix()
              if rel.startswith("docs/_internal/") or rel.startswith("docs/_assets/") or rel.startswith("docs/_drafts/"):
                  continue
              all_docs.append(rel)
          all_set = set(all_docs)
          links = {rel: set() for rel in all_docs}
          md_link_re = re.compile(r"\\[[^\\]]+\\]\\(([^)]+)\\)")
          for rel in all_docs:
              src_path = root / rel
              text = src_path.read_text()
              for target in md_link_re.findall(text):
                  target = target.split("#")[0].strip()
                  if not target or target.startswith("http://") or target.startswith("https://") or target.startswith("mailto:"):
                      continue
                  cand = (src_path.parent / target).resolve()
                  try:
                      rel_target = cand.relative_to(root).as_posix()
                  except Exception:
                      continue
                  if rel_target in all_set:
                      links[rel].add(rel_target)
          reachable = set()
          queue = ["docs/index.md", "docs/start-here.md"]
          while queue:
              cur = queue.pop(0)
              if cur in reachable:
                  continue
              reachable.add(cur)
              for nxt in sorted(links.get(cur, [])):
                  if nxt not in reachable:
                      queue.append(nxt)
          orphans = sorted([p for p in all_docs if p not in reachable])
          orphan_hints = []
          for orphan in orphans:
              parent = pathlib.Path(orphan).parent.as_posix()
              parent_index = f"{parent}/index.md" if parent != "docs" else "docs/index.md"
              if parent_index == orphan:
                  parent_index = "docs/index.md"
              orphan_hints.append({
                  "path": orphan,
                  "suggested_link_source": parent_index,
                  "reason": "orphan page is unreachable from docs/index.md or docs/start-here.md",
              })
          # 6) Duplicate topic collisions (top 20 by Jaccard similarity).
          tokens = {}
          for rel in all_docs:
              text = (root / rel).read_text()
              words = re.findall(r"[A-Za-z0-9]{5,}", text.lower())
              tokens[rel] = set(words)
          collisions = []
          docs_list = sorted(all_docs)
          for i, left in enumerate(docs_list):
              for right in docs_list[i + 1:]:
                  lt = tokens[left]
                  rt = tokens[right]
                  if not lt or not rt:
                      continue
                  inter = len(lt & rt)
                  if inter == 0:
                      continue
                  union = len(lt | rt)
                  score = inter / union
                  if score >= 0.35:
                      collisions.append({"left": left, "right": right, "similarity": round(score, 3), "shared_token_count": inter})
          collisions.sort(key=lambda row: (-row["similarity"], row["left"], row["right"]))
          top_collisions = collisions[:20]
          payload = {
              "schema_version": 1,
              "kind": "docs_navigation_and_semantics_contracts",
              "status": "ok",
              "directory_index_missing": [],
              "orphan_hints": orphan_hints,
              "duplicate_topic_collisions_top20": top_collisions,
              "three_reader_tracks": expected_tracks,
              "spine_pages_checked": spine_pages,
              "required_glossary_terms": required_glossary_terms,
          }
          (report_root / "docs-navigation-and-semantics-contracts.json").write_text(json.dumps(payload, indent=2) + "\\n")
          PY
          cargo run -q -p bijux-dev-atlas -- docs shrink-report --strict --format json > "${report_root}/docs-shrink-report.json"
          ! rg -n "(AKIA|BEGIN PRIVATE KEY|SECRET|TOKEN=)" "${report_root}/ci-fast.json"
          printf "- lane: ci-fast\n- report: %s\n- cache registry hit: %s\n- cache target hit: %s\n" \
            "${report_root}/ci-fast.json" "${{ steps.cache-registry.outputs.cache-hit || 'false' }}" "${{ steps.cache-target.outputs.cache-hit || 'false' }}" > "artifacts/${RUN_ID}/summary.md"
          printf "\n- contracts all: %s\n- docs required: %s\n- docs required membership: %s\n- configs required: %s\n- configs required membership: %s\n- make required: %s\n- make required membership: %s\n- repo required: %s\n- repo required membership: %s\n- repo integrity summary: %s\n- contracts runtime static: %s\n- contracts control-plane static: %s\n- contracts configs static: %s\n- contracts make static: %s\n- contracts ops static: %s\n- contracts docker static: %s\n- coverage: %s\n" \
            "${contracts_root}/all.json" "${contracts_root}/docs-required.json" "${report_root}/suite-membership-docs_required.json" "${contracts_root}/configs-required.json" "${report_root}/suite-membership-configs_required.json" "${contracts_root}/make-required.json" "${report_root}/suite-membership-make_required.json" "${contracts_root}/repo-required.json" "${report_root}/suite-membership-repo_required.json" "${report_root}/repo-integrity-summary.json" "${contracts_root}/runtime-static.json" "${contracts_root}/control-plane-static.json" "${contracts_root}/configs-static.json" "${contracts_root}/make-static.txt" "${contracts_root}/ops-static.json" "${contracts_root}/docker-static.json" "${contracts_root}/coverage-summary.json" >> "artifacts/${RUN_ID}/summary.md"
          printf "- repo surface diff: %s\n- root change report: %s\n- governance validate: %s\n- governance coverage diff: %s\n- semantic drift summary: %s\n- make surface drift: %s\n- deterministic check order: %s\n- check metadata catalog: %s\n- artifact schema check: %s\n- artifact naming and size: %s\n- docs golden workflow verification: %s\n- docs link allowlist policy: %s\n- alert runbook slo load e2e mapping: %s\n- registry group coverage: %s\n- contract coverage heatmap: %s\n- missing proof report: %s\n- proof debt ledger: %s\n- contract flake detector: %s\n- contract performance budget: %s\n- docs entrypoint and budget contracts: %s\n- docs navigation and semantics contracts: %s\n- docs shrink report: %s\n" "${report_root}/repo-surface-diff.json" "${report_root}/root-change-report.json" "${report_root}/governance-validate.json" "${report_root}/governance-coverage-diff.json" "${report_root}/semantic-drift-summary.json" "${report_root}/make-surface-drift.json" "${report_root}/deterministic-check-order.json" "${report_root}/check-metadata-catalog.json" "${report_root}/artifact-schema-version-check.json" "${report_root}/artifact-naming-and-size.json" "${report_root}/docs-golden-workflow-verification.json" "${report_root}/docs-link-allowlist-policy.json" "${report_root}/ops-alert-runbook-slo-load-e2e-map.json" "${report_root}/registry-group-coverage.json" "${report_root}/contract-coverage-heatmap.json" "${report_root}/missing-proof-report.json" "${report_root}/proof-debt-ledger.json" "${report_root}/contract-flake-detector.json" "${report_root}/contract-performance-budget.json" "${report_root}/docs-entrypoint-and-budget-contracts.json" "${report_root}/docs-navigation-and-semantics-contracts.json" "${report_root}/docs-shrink-report.json" >> "artifacts/${RUN_ID}/summary.md"
          if [ -f "${contracts_root}/root-strict.json" ]; then
            printf -- "- root strict: %s\n" "${contracts_root}/root-strict.json" >> "artifacts/${RUN_ID}/summary.md"
          fi
          if ! git diff --quiet; then
            git status --short > "${report_root}/repo-write-drift.txt"
            jq -n --rawfile dirty "${report_root}/repo-write-drift.txt" '{
              schema_version: 1,
              kind: "repo_write_drift",
              status: "failed",
              dirty: ($dirty | split("\n") | map(select(length>0)))
            }' > "${report_root}/repo-write-drift.json"
            echo "repository changed during checks; checks must write only to artifacts" >&2
            exit 1
          fi
          jq -n '{schema_version:1,kind:"repo_write_drift",status:"ok",dirty:[]}' > "${report_root}/repo-write-drift.json"
          cat "artifacts/${RUN_ID}/summary.md" >> "${GITHUB_STEP_SUMMARY}"
      - name: Save cargo registry cache
        if: success()
        uses: actions/cache/save@0057852bfaa89a56745cba8c7296529d2fc39830
        with:
          path: |
            .cache/cargo/home/ci-pr-minimal/registry
            .cache/cargo/home/ci-pr-minimal/git
          key: cargo-registry-pr-${{ runner.os }}-1.84.1-${{ hashFiles('Cargo.lock') }}
      - name: Save cargo target cache
        if: success()
        uses: actions/cache/save@0057852bfaa89a56745cba8c7296529d2fc39830
        with:
          path: .cache/cargo/target/ci-pr-minimal
          key: cargo-target-pr-minimal-${{ runner.os }}-1.84.1-${{ hashFiles('Cargo.lock') }}
      - uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02
        if: always()
        with:
          name: ci-pr-minimal-${{ env.RUN_ID }}
          path: artifacts/${{ env.RUN_ID }}
          retention-days: 5

  validate-pr:
    if: needs.route-changes.outputs.docs_only != 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 45
    needs: route-changes
    env:
      NETWORK_ACCESS: required
      ISO_ROOT: artifacts/isolates/ci-pr
      CARGO_TARGET_DIR: .cache/cargo/target/ci-pr
      CARGO_HOME: .cache/cargo/home/ci-pr
      NEXTEST_CACHE_DIR: .cache/cargo/nextest/ci-pr
      TMPDIR: artifacts/isolates/ci-pr/tmp
      TMP: artifacts/isolates/ci-pr/tmp
      TEMP: artifacts/isolates/ci-pr/tmp
    steps:
      - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5
      - uses: dtolnay/rust-toolchain@631a55b12751854ce901bb631d5902ceb48146f7
        with:
          toolchain: 1.84.1
      - name: Restore cargo registry cache
        id: cache-registry
        uses: actions/cache/restore@0057852bfaa89a56745cba8c7296529d2fc39830
        with:
          path: |
            .cache/cargo/home/ci-pr/registry
            .cache/cargo/home/ci-pr/git
          key: cargo-registry-pr-${{ runner.os }}-1.84.1-${{ hashFiles('Cargo.lock') }}
      - name: Restore cargo target cache
        id: cache-target
        uses: actions/cache/restore@0057852bfaa89a56745cba8c7296529d2fc39830
        with:
          path: .cache/cargo/target/ci-pr
          key: cargo-target-pr-${{ runner.os }}-1.84.1-${{ hashFiles('Cargo.lock') }}
      - name: Install docs toolchain
        shell: bash
        run: |
          set -euo pipefail
          python3 -m pip install --upgrade pip
          python3 -m pip install -r configs/docs/requirements.lock.txt
      - name: Run PR lane
        shell: bash
        run: |
          set -euo pipefail
          report_root="artifacts/${RUN_ID}/reports"
          logs_root="artifacts/${RUN_ID}/logs/ci-pr"
          mkdir -p "${report_root}" "${logs_root}"
          make doctor > "${report_root}/doctor.json" 2> "${logs_root}/doctor.stderr.log"
          cargo run -q -p bijux-dev-atlas -- check tree-budgets --format json > "${report_root}/tree-budgets.json" 2> "${logs_root}/tree-budgets.stderr.log"
          make help > "${logs_root}/make-help.stdout.log" 2> "${logs_root}/make-help.stderr.log"
          make make-target-list > "${logs_root}/make-target-list.stdout.log" 2> "${logs_root}/make-target-list.stderr.log"
          cp make/target-list.json "${report_root}/make-target-list.json"
          cargo run -q -p bijux-dev-atlas -- check run --domain root --tag lint --format json > "${logs_root}/lint-policy-report.stdout.log" 2> "${logs_root}/lint-policy-report.stderr.log"
          printf 'workspace_lints_file=Cargo.toml\nclippy_conf_dir=configs/rust\n' > "${report_root}/effective-clippy-policy.txt"
          cargo run -q -p bijux-dev-atlas -- check run --domain root --tag lint --format json > "${logs_root}/lint-policy-enforce.stdout.log" 2> "${logs_root}/lint-policy-enforce.stderr.log"
          CLIPPY_CONF_DIR=configs/rust cargo clippy --workspace --all-targets --all-features --locked --message-format=json -- -D warnings > "${report_root}/clippy.json" 2> "${logs_root}/lint-clippy-json.stderr.log"
          cargo tree -p bijux-atlas-store --edges normal --no-default-features --features backend-local > "${report_root}/store-deps-backend-local.txt"
          cargo tree -p bijux-atlas-store --edges normal --no-default-features --features backend-s3 > "${report_root}/store-deps-backend-s3.txt"
          local_dep_count="$(wc -l < "${report_root}/store-deps-backend-local.txt")"
          s3_dep_count="$(wc -l < "${report_root}/store-deps-backend-s3.txt")"
          [ "${s3_dep_count}" -gt "${local_dep_count}" ]
          printf '{"schema_version":1,"kind":"store_backend_dependency_surface","backend_local_count":%s,"backend_s3_count":%s}\n' "${local_dep_count}" "${s3_dep_count}" > "${report_root}/store-deps-diff.json"
          cp ops/inventory/release-build-profile.json "${report_root}/store-release-backend-profile.json"
          cargo check -q -p bijux-atlas-store --no-default-features --features backend-local > "${logs_root}/store-backend-local-check.stdout.log" 2> "${logs_root}/store-backend-local-check.stderr.log"
          cargo check -q -p bijux-atlas-store --no-default-features --features backend-s3 > "${logs_root}/store-backend-s3-check.stdout.log" 2> "${logs_root}/store-backend-s3-check.stderr.log"
          cargo test -q -p bijux-atlas-store --no-default-features --features backend-local local_backend_roundtrip_is_hermetic -- --exact > "${logs_root}/store-backend-local-test.stdout.log" 2> "${logs_root}/store-backend-local-test.stderr.log"
          cargo test -q -p bijux-atlas-store --no-default-features --features backend-s3 http_backend_reads_from_hermetic_cached_objects -- --exact > "${logs_root}/store-backend-s3-test.stdout.log" 2> "${logs_root}/store-backend-s3-test.stderr.log"
          cargo fmt --all -- --check --config-path configs/rust/rustfmt.toml > "${logs_root}/fmt-check.stdout.log" 2> "${logs_root}/fmt-check.stderr.log"
          cargo run -q -p bijux-dev-atlas -- configs verify --format json > "${report_root}/configs-verify.json" 2> "${logs_root}/configs-verify.stderr.log"
          cargo test --workspace --locked > "${logs_root}/workspace-test.stdout.log" 2> "${logs_root}/workspace-test.stderr.log"
          cargo test -q -p bijux-atlas-api openapi_snapshot_is_deterministic_and_matches_committed_contract -- --exact > "${logs_root}/openapi-snapshot.stdout.log" 2> "${logs_root}/openapi-snapshot.stderr.log"
          cargo test -q -p bijux-atlas-api openapi_hash_matches_pinned_contract -- --exact > "${logs_root}/openapi-hash.stdout.log" 2> "${logs_root}/openapi-hash.stderr.log"
          cargo test -q -p bijux-atlas-server runtime_startup_config_contract_artifacts_match_generated > "${logs_root}/runtime-startup-config-contract.stdout.log" 2> "${logs_root}/runtime-startup-config-contract.stderr.log"
          mkdocs build --strict > "${report_root}/mkdocs-build.json" 2> "${logs_root}/mkdocs-build.stderr.log"
          cargo run -q -p bijux-dev-atlas -- docs external-links --allow-network --format json > "${report_root}/docs-linkcheck.json" 2> "${logs_root}/docs-linkcheck.stderr.log"
          ARTIFACT_ROOT="${report_root}" RUN_ID="${RUN_ID}" make docker-contracts > "${logs_root}/docker-contracts-static.stdout.log" 2> "${logs_root}/docker-contracts-static.stderr.log"
          make ops-contracts > "${logs_root}/ops-contracts-static.stdout.log" 2> "${logs_root}/ops-contracts-static.stderr.log"
          make ops-pr > "${report_root}/ci-pr.json" 2> "${logs_root}/ci-pr.stderr.log"
          cargo run -q -p bijux-dev-atlas -- check list --suite ci_pr --include-internal --include-slow --format json > "${report_root}/suite-membership-ci_pr.json"
          ! rg -n "(AKIA|BEGIN PRIVATE KEY|SECRET|TOKEN=)" "${report_root}/ci-pr.json"
          printf "- lane: ci-pr\n- report: %s\n- docs build: %s\n- docs linkcheck: %s\n- cache registry hit: %s\n- cache target hit: %s\n" \
            "${report_root}/ci-pr.json" "${report_root}/mkdocs-build.json" "${report_root}/docs-linkcheck.json" "${{ steps.cache-registry.outputs.cache-hit || 'false' }}" "${{ steps.cache-target.outputs.cache-hit || 'false' }}" > "artifacts/${RUN_ID}/summary.md"
          cat "artifacts/${RUN_ID}/summary.md" >> "${GITHUB_STEP_SUMMARY}"
      - name: Save cargo registry cache
        if: success()
        uses: actions/cache/save@0057852bfaa89a56745cba8c7296529d2fc39830
        with:
          path: |
            .cache/cargo/home/ci-pr/registry
            .cache/cargo/home/ci-pr/git
          key: cargo-registry-pr-${{ runner.os }}-1.84.1-${{ hashFiles('Cargo.lock') }}
      - name: Save cargo target cache
        if: success()
        uses: actions/cache/save@0057852bfaa89a56745cba8c7296529d2fc39830
        with:
          path: .cache/cargo/target/ci-pr
          key: cargo-target-pr-${{ runner.os }}-1.84.1-${{ hashFiles('Cargo.lock') }}
      - uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02
        if: always()
        with:
          name: ci-pr-${{ env.RUN_ID }}
          path: artifacts/${{ env.RUN_ID }}
          retention-days: 5

  supply-chain:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    env:
      NETWORK_ACCESS: required
      ISO_ROOT: artifacts/isolates/ci-supply-chain
      CARGO_TARGET_DIR: .cache/cargo/target/ci-supply-chain
      CARGO_HOME: .cache/cargo/home/ci-supply-chain
      NEXTEST_CACHE_DIR: .cache/cargo/nextest/ci-supply-chain
      TMPDIR: artifacts/isolates/ci-supply-chain/tmp
      TMP: artifacts/isolates/ci-supply-chain/tmp
      TEMP: artifacts/isolates/ci-supply-chain/tmp
    steps:
      - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5
      - uses: dtolnay/rust-toolchain@631a55b12751854ce901bb631d5902ceb48146f7
        with:
          toolchain: 1.84.1
      - name: Restore cargo registry cache
        id: cache-registry
        uses: actions/cache/restore@0057852bfaa89a56745cba8c7296529d2fc39830
        with:
          path: |
            .cache/cargo/home/ci-supply-chain/registry
            .cache/cargo/home/ci-supply-chain/git
          key: cargo-registry-pr-${{ runner.os }}-1.84.1-${{ hashFiles('Cargo.lock') }}
      - name: Restore cargo target cache
        id: cache-target
        uses: actions/cache/restore@0057852bfaa89a56745cba8c7296529d2fc39830
        with:
          path: .cache/cargo/target/ci-supply-chain
          key: cargo-target-pr-supply-${{ runner.os }}-1.84.1-${{ hashFiles('Cargo.lock') }}
      - name: Run audit lane
        shell: bash
        run: |
          set -euo pipefail
          report_root="artifacts/${RUN_ID}/reports"
          logs_root="artifacts/${RUN_ID}/logs/ci-supply-chain"
          mkdir -p "${report_root}" "${logs_root}"
          cargo install --locked cargo-deny >/dev/null 2> "${logs_root}/cargo-deny-install.stderr.log"
          cargo deny --config configs/security/deny.toml check > "${logs_root}/deny.stdout.log" 2> "${logs_root}/deny.stderr.log"
          cargo audit > "${logs_root}/audit.stdout.log" 2> "${logs_root}/audit.stderr.log"
          printf '{"schema_version":1,"kind":"supply_chain_report_v1","commands":["cargo deny --config configs/security/deny.toml check","cargo audit"],"status":"ok"}\n' > "${report_root}/supply-chain.json"
          ! rg -n "(AKIA|BEGIN PRIVATE KEY|SECRET|TOKEN=)" "${report_root}/supply-chain.json"
          printf "- lane: supply-chain\n- report: %s\n- cache registry hit: %s\n- cache target hit: %s\n" \
            "${report_root}/supply-chain.json" "${{ steps.cache-registry.outputs.cache-hit || 'false' }}" "${{ steps.cache-target.outputs.cache-hit || 'false' }}" > "artifacts/${RUN_ID}/summary.md"
          cat "artifacts/${RUN_ID}/summary.md" >> "${GITHUB_STEP_SUMMARY}"
      - name: Save cargo registry cache
        if: success()
        uses: actions/cache/save@0057852bfaa89a56745cba8c7296529d2fc39830
        with:
          path: |
            .cache/cargo/home/ci-supply-chain/registry
            .cache/cargo/home/ci-supply-chain/git
          key: cargo-registry-pr-${{ runner.os }}-1.84.1-${{ hashFiles('Cargo.lock') }}
      - name: Save cargo target cache
        if: success()
        uses: actions/cache/save@0057852bfaa89a56745cba8c7296529d2fc39830
        with:
          path: .cache/cargo/target/ci-supply-chain
          key: cargo-target-pr-supply-${{ runner.os }}-1.84.1-${{ hashFiles('Cargo.lock') }}
      - uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02
        if: always()
        with:
          name: ci-supply-chain-${{ env.RUN_ID }}
          path: artifacts/${{ env.RUN_ID }}
          retention-days: 5

  workflow-policy:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5
      - name: Check action pinning policy
        shell: bash
        run: |
          set -euo pipefail
          violations="$(rg -n "uses:\s+[^@\s]+@(main|master|HEAD|latest)$" .github/workflows || true)"
          if [ -n "${violations}" ]; then
            echo "unacceptable floating action references found"
            echo "${violations}"
            exit 1
          fi
      - name: Enforce no direct script execution in workflows
        shell: bash
        run: |
          set -euo pipefail
          ! rg -n "bash +scripts/|python(3)? +scripts/" .github/workflows
      - name: Enforce no direct external tool invocations
        shell: bash
        run: |
          set -euo pipefail
          ! rg -n "(^|\s)(helm|kubectl|k6|kind)(\s|$)" .github/workflows \
            -g '!ops-integration-kind.yml' -g '!ci-pr.yml'
      - name: Enforce dependabot config contract
        shell: bash
        run: |
          set -euo pipefail
          test -f .github/dependabot.yml
          rg -n 'package-ecosystem:\s+"cargo"' .github/dependabot.yml
          rg -n 'package-ecosystem:\s+"github-actions"' .github/dependabot.yml
      - name: Enforce docs ownership contract
        shell: bash
        run: |
          set -euo pipefail
          test -f .github/CODEOWNERS
          rg -n '^/docs/ +@bijan' .github/CODEOWNERS
          rg -n '^/crates/\*/docs/ +@bijan' .github/CODEOWNERS
      - name: Enforce cache size cap
        shell: bash
        run: |
          set -euo pipefail
          if [ -d .cache/cargo/target ]; then
            size_kb="$(du -sk .cache/cargo/target | awk '{print $1}')"
            [ "${size_kb}" -le 4194304 ]
          fi

  docker-runtime-image:
    if: needs.route-changes.outputs.docs_only != 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 25
    needs: route-changes
    env:
      ISO_ROOT: artifacts/isolates/ci-pr-docker
      TMPDIR: artifacts/isolates/ci-pr-docker/tmp
      TMP: artifacts/isolates/ci-pr-docker/tmp
      TEMP: artifacts/isolates/ci-pr-docker/tmp
    steps:
      - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5
      - uses: dtolnay/rust-toolchain@631a55b12751854ce901bb631d5902ceb48146f7
        with:
          toolchain: 1.84.1
      - name: Run docker gate lane
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p "artifacts/${RUN_ID}"
          make docker-contracts > "artifacts/${RUN_ID}/docker-contracts.stdout.log" 2> "artifacts/${RUN_ID}/docker-contracts.stderr.log"
      - uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02
        if: always()
        with:
          name: docker-runtime-${{ env.RUN_ID }}
          path: artifacts/${{ env.RUN_ID }}
          retention-days: 5
