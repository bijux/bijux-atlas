name: ci-pr

on:
  pull_request:
  merge_group:

permissions:
  contents: read

concurrency:
  group: ci-pr-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

env:
  RUN_ID: pr-${{ github.event.pull_request.number || github.run_id }}-${{ github.run_attempt }}
  CARGO_TERM_COLOR: always

jobs:
  route-changes:
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      docs_only: ${{ steps.filter.outputs.docs_only }}
    steps:
      - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5
      - id: filter
        uses: dorny/paths-filter@de90cc6fb38fc0963ad72b210f1f284cd68cea36
        with:
          filters: |
            docs_only:
              - 'docs/**'
              - 'mkdocs.yml'
              - '.github/**'
              - 'configs/docs/**'
              - 'configs/openapi/**'
              - 'make/docs.mk'

  minimal-root-policies:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: route-changes
    env:
      NETWORK_ACCESS: required
      ISO_ROOT: artifacts/isolates/ci-pr-minimal
      CARGO_TARGET_DIR: .cache/cargo/target/ci-pr-minimal
      CARGO_HOME: .cache/cargo/home/ci-pr-minimal
      NEXTEST_CACHE_DIR: .cache/cargo/nextest/ci-pr-minimal
      TMPDIR: artifacts/isolates/ci-pr-minimal/tmp
      TMP: artifacts/isolates/ci-pr-minimal/tmp
      TEMP: artifacts/isolates/ci-pr-minimal/tmp
    steps:
      - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5
      - uses: dtolnay/rust-toolchain@631a55b12751854ce901bb631d5902ceb48146f7
        with:
          toolchain: 1.84.1
      - name: Prepare run artifact directory
        shell: bash
        run: |
          set -euo pipefail
          rm -rf "artifacts/${RUN_ID}"
          mkdir -p "artifacts/${RUN_ID}"
      - name: Restore cargo registry cache
        id: cache-registry
        uses: actions/cache/restore@0057852bfaa89a56745cba8c7296529d2fc39830
        with:
          path: |
            .cache/cargo/home/ci-pr-minimal/registry
            .cache/cargo/home/ci-pr-minimal/git
          key: cargo-registry-pr-${{ runner.os }}-1.84.1-${{ hashFiles('Cargo.lock') }}
      - name: Restore cargo target cache
        id: cache-target
        uses: actions/cache/restore@0057852bfaa89a56745cba8c7296529d2fc39830
        with:
          path: .cache/cargo/target/ci-pr-minimal
          key: cargo-target-pr-minimal-${{ runner.os }}-1.84.1-${{ hashFiles('Cargo.lock') }}
      - name: Run minimal lane
        shell: bash
        run: |
          set -euo pipefail
          report_root="artifacts/${RUN_ID}/reports"
          logs_root="artifacts/${RUN_ID}/logs/ci-pr-minimal"
          mkdir -p "${report_root}" "${logs_root}"
          contracts_root="${report_root}/contracts"
          mkdir -p "${contracts_root}"
          cargo run -q -p bijux-dev-atlas -- contracts self-check --format json > "${contracts_root}/self-check.json" 2> "${logs_root}/contracts-self-check.stderr.log"
          make doctor > "${report_root}/doctor.json" 2> "${logs_root}/doctor.stderr.log"
          cargo run -q -p bijux-dev-atlas -- check tree-budgets --format json > "${report_root}/tree-budgets.json" 2> "${logs_root}/tree-budgets.stderr.log"
          make help > "${logs_root}/make-help.stdout.log" 2> "${logs_root}/make-help.stderr.log"
          make make-target-list > "${logs_root}/make-target-list.stdout.log" 2> "${logs_root}/make-target-list.stderr.log"
          cp make/target-list.json "${report_root}/make-target-list.json"
          if [ "${GITHUB_EVENT_NAME}" = "merge_group" ]; then
            ARTIFACT_ROOT="${report_root}" RUN_ID="${RUN_ID}" make contracts-merge > "${contracts_root}/all.json" 2> "${logs_root}/contracts-merge.stderr.log"
          else
            ARTIFACT_ROOT="${report_root}" RUN_ID="${RUN_ID}" make contracts-pr > "${contracts_root}/all.json" 2> "${logs_root}/contracts-pr.stderr.log"
          fi
          cargo run -q -p bijux-dev-atlas -- check run --domain root --tag lint --format json > "${logs_root}/lint-policy-report.stdout.log" 2> "${logs_root}/lint-policy-report.stderr.log"
          printf 'workspace_lints_file=Cargo.toml\nclippy_conf_dir=configs/rust\n' > "${report_root}/effective-clippy-policy.txt"
          cargo run -q -p bijux-dev-atlas -- check run --domain root --tag lint --format json > "${logs_root}/lint-policy-enforce.stdout.log" 2> "${logs_root}/lint-policy-enforce.stderr.log"
          CLIPPY_CONF_DIR=configs/rust cargo clippy --workspace --all-targets --all-features --locked --message-format=json -- -D warnings > "${report_root}/clippy.json" 2> "${logs_root}/lint-clippy-json.stderr.log"
          cargo tree -p bijux-atlas-store --edges normal --no-default-features --features backend-local > "${report_root}/store-deps-backend-local.txt"
          cargo tree -p bijux-atlas-store --edges normal --no-default-features --features backend-s3 > "${report_root}/store-deps-backend-s3.txt"
          local_dep_count="$(wc -l < "${report_root}/store-deps-backend-local.txt")"
          s3_dep_count="$(wc -l < "${report_root}/store-deps-backend-s3.txt")"
          [ "${s3_dep_count}" -gt "${local_dep_count}" ]
          printf '{"schema_version":1,"kind":"store_backend_dependency_surface","backend_local_count":%s,"backend_s3_count":%s}\n' "${local_dep_count}" "${s3_dep_count}" > "${report_root}/store-deps-diff.json"
          cp ops/inventory/release-build-profile.json "${report_root}/store-release-backend-profile.json"
          cargo check -q -p bijux-atlas-store --no-default-features --features backend-local > "${logs_root}/store-backend-local-check.stdout.log" 2> "${logs_root}/store-backend-local-check.stderr.log"
          cargo check -q -p bijux-atlas-store --no-default-features --features backend-s3 > "${logs_root}/store-backend-s3-check.stdout.log" 2> "${logs_root}/store-backend-s3-check.stderr.log"
          cargo test -q -p bijux-atlas-store --no-default-features --features backend-local local_backend_roundtrip_is_hermetic -- --exact > "${logs_root}/store-backend-local-test.stdout.log" 2> "${logs_root}/store-backend-local-test.stderr.log"
          cargo test -q -p bijux-atlas-store --no-default-features --features backend-s3 http_backend_reads_from_hermetic_cached_objects -- --exact > "${logs_root}/store-backend-s3-test.stdout.log" 2> "${logs_root}/store-backend-s3-test.stderr.log"
          cargo fmt --all -- --check --config-path configs/rust/rustfmt.toml > "${logs_root}/fmt-check.stdout.log" 2> "${logs_root}/fmt-check.stderr.log"
          cargo run -q -p bijux-dev-atlas -- configs verify --format json > "${report_root}/configs-verify.json" 2> "${logs_root}/configs-verify.stderr.log"
          cargo run -q -p bijux-dev-atlas -- check run --suite docs:required --include-internal --include-slow --format json > "${contracts_root}/docs-required.json" 2> "${logs_root}/docs-required.stderr.log"
          cargo run -q -p bijux-dev-atlas -- check run --suite configs:required --include-internal --include-slow --format json > "${contracts_root}/configs-required.json" 2> "${logs_root}/configs-required.stderr.log"
          cargo run -q -p bijux-dev-atlas -- check run --suite make:required --include-internal --include-slow --format json > "${contracts_root}/make-required.json" 2> "${logs_root}/make-required.stderr.log"
          cargo run -q -p bijux-dev-atlas -- check run --suite repo_required --include-internal --include-slow --allow-git --format json > "${contracts_root}/repo-required.json" 2> "${logs_root}/repo-required.stderr.log"
          cargo run -q -p bijux-dev-atlas -- check list --suite docs:required --include-internal --include-slow --format json > "${report_root}/suite-membership-docs_required.json"
          cargo run -q -p bijux-dev-atlas -- check list --suite configs:required --include-internal --include-slow --format json > "${report_root}/suite-membership-configs_required.json"
          cargo run -q -p bijux-dev-atlas -- check list --suite make:required --include-internal --include-slow --format json > "${report_root}/suite-membership-make_required.json"
          cargo run -q -p bijux-dev-atlas -- check list --suite repo_required --include-internal --include-slow --format json > "${report_root}/suite-membership-repo_required.json"
          jq -n \
            --arg run_id "${RUN_ID}" \
            --arg source "docs/_internal/contracts/repo-laws.json" \
            --arg suite "repo_required" \
            --slurpfile suite_members "${report_root}/suite-membership-repo_required.json" \
            --slurpfile run_report "${contracts_root}/repo-required.json" \
            '{schema_version:1,kind:"repo_integrity_summary",run_id:$run_id,laws_source:$source,required_suite:$suite,checks_total:($suite_members[0].checks|length),counts:$run_report[0].counts,p0_checks:$suite_members[0].checks|map(.id)}' \
            > "${report_root}/repo-integrity-summary.json"
          base_ref="${GITHUB_BASE_REF:-main}"
          base_sha="$(git merge-base HEAD "origin/${base_ref}" || true)"
          if [ -z "${base_sha}" ]; then
            base_sha="$(git rev-parse HEAD~1)"
          fi
          git diff --name-status "${base_sha}...HEAD" > "${report_root}/repo-surface-diff.txt"
          export REPORT_ROOT="${report_root}"
          export BASE_SHA="${base_sha}"
          python - <<'PY'
          import json, pathlib, os
          root = pathlib.Path(os.environ["REPORT_ROOT"])
          lines = [ln.strip() for ln in (root / "repo-surface-diff.txt").read_text().splitlines() if ln.strip()]
          rows = []
          root_rows = []
          for line in lines:
            parts = line.split("\t")
            status = parts[0]
            paths = [p for p in parts[1:] if p]
            rows.append({"status": status, "paths": paths})
            for p in paths:
              if "/" not in p:
                root_rows.append({"status": status, "path": p})
          (root / "repo-surface-diff.json").write_text(json.dumps({
              "schema_version": 1,
              "kind": "repo_surface_diff",
              "base": os.environ.get("BASE_SHA", ""),
              "head": os.environ.get("GITHUB_SHA", ""),
              "changes": rows,
          }, indent=2) + "\n")
          (root / "root-change-report.json").write_text(json.dumps({
              "schema_version": 1,
              "kind": "root_change_report",
              "base": os.environ.get("BASE_SHA", ""),
              "head": os.environ.get("GITHUB_SHA", ""),
              "root_changes": root_rows,
          }, indent=2) + "\n")
          PY
          root_changed_count="$(jq -r '.root_changes | length' "${report_root}/root-change-report.json")"
          if [ "${root_changed_count}" -gt 0 ]; then
            cargo run -q -p bijux-dev-atlas -- contracts root --mode static --format json --artifacts-root "${contracts_root}" > "${contracts_root}/root-strict.json" 2> "${logs_root}/root-strict.stderr.log"
          fi
          semantic_target_count="$(jq -r '[.root_changes[] | select(.path == \"README.md\" or .path == \"CONTRACT.md\")] | length' "${report_root}/root-change-report.json")"
          if [ "${semantic_target_count}" -gt 0 ] && [ "${GITHUB_EVENT_NAME}" = "pull_request" ]; then
            pr_body="$(jq -r '.pull_request.body // \"\"' \"${GITHUB_EVENT_PATH}\")"
            has_label="$(jq -r '[.pull_request.labels[]?.name | select(. == \"root-semantic-diff-approved\")] | length' \"${GITHUB_EVENT_PATH}\")"
            if ! printf '%s' "${pr_body}" | grep -q '\\[root-semantic-diff\\]' && [ "${has_label}" -eq 0 ]; then
              echo \"missing required root semantic diff marker: add [root-semantic-diff] to PR description or label root-semantic-diff-approved\" >&2
              exit 1
            fi
          fi
          cargo run -q -p bijux-dev-atlas -- governance validate --format json > "${report_root}/governance-validate.json" 2> "${logs_root}/governance-validate.stderr.log"
          coverage_current="$(jq -r '.coverage_percent' artifacts/governance/governance-coverage.json)"
          coverage_baseline="$(jq -r '.coverage_percent_min' governance/coverage-baseline.json)"
          delta="$(awk -v c="${coverage_current}" -v b="${coverage_baseline}" 'BEGIN { printf \"%.2f\", c-b }')"
          status="ok"
          if awk -v c="${coverage_current}" -v b="${coverage_baseline}" 'BEGIN { exit !(c < b) }'; then
            status="failed"
            echo "governance coverage dropped below baseline: current=${coverage_current} baseline=${coverage_baseline}" >&2
            exit 1
          fi
          jq -n \
            --arg status "${status}" \
            --argjson current "${coverage_current}" \
            --argjson baseline "${coverage_baseline}" \
            --arg delta "${delta}" \
            '{
              schema_version: 1,
              kind: "governance_coverage_diff",
              status: $status,
              baseline_percent: $baseline,
              current_percent: $current,
              delta_percent: ($delta | tonumber)
            }' > "${report_root}/governance-coverage-diff.json"
          changed_list="$(git diff --name-only "${base_sha}...HEAD")"
          changed_file() {
            printf '%s\n' "${changed_list}" | grep -qx "$1"
          }
          python - <<'PY'
          import json, os, pathlib, subprocess
          base = os.environ.get("BASE_SHA", "").strip()
          head = os.environ.get("GITHUB_SHA", "").strip()
          report_root = pathlib.Path(os.environ["REPORT_ROOT"])
          changed = []
          if base:
              out = subprocess.check_output(["git", "diff", "--name-only", f"{base}...HEAD"], text=True)
              changed = [line.strip() for line in out.splitlines() if line.strip()]
          def subset(prefixes):
              return sorted([p for p in changed if any(p.startswith(px) for px in prefixes)])
          reports = {
              "schema_version": 1,
              "kind": "semantic_drift_reports",
              "base": base,
              "head": head,
              "configs": {"changed": subset(["configs/", "crates/bijux-atlas-server/docs/generated/effective-config.snapshot.json", "docs/reference/contracts/schemas/CONFIG_KEYS.json"])},
              "docs": {"changed": subset(["docs/"])},
              "make": {"changed": subset(["make/", "configs/ops/make-target-registry.json", "make/target-list.json"])},
              "docker": {"changed": subset(["docker/", "docker/images.manifest.json", "docker/policy.json"])},
              "ops_readiness": {"changed": subset(["ops/observe/readiness.json", "ops/report/evidence-levels.json", "ops/report/generated/readiness-score.json", "ops/report/generated/release-evidence-bundle.json"])},
          }
          for domain in ["configs", "docs", "make", "docker", "ops_readiness"]:
              reports[domain]["status"] = "changed" if reports[domain]["changed"] else "unchanged"
              reports[domain]["count"] = len(reports[domain]["changed"])
              path = report_root / f"semantic-drift-{domain}.json"
              path.write_text(json.dumps({
                  "schema_version": 1,
                  "kind": f"semantic_drift_{domain}",
                  "base": base,
                  "head": head,
                  "status": reports[domain]["status"],
                  "changed": reports[domain]["changed"],
                  "count": reports[domain]["count"],
              }, indent=2) + "\n")
          (report_root / "semantic-drift-summary.json").write_text(json.dumps(reports, indent=2) + "\n")
          PY
          pr_body="$(jq -r '.pull_request.body // \"\"' "${GITHUB_EVENT_PATH}")"
          if changed_file "configs/openapi/v1/openapi.snapshot.json"; then
            if ! printf '%s' "${pr_body}" | grep -Eq '\[api-compat:(non-breaking|breaking)\]'; then
              echo "OpenAPI snapshot changed but compatibility marker is missing; add [api-compat:non-breaking] or [api-compat:breaking] in PR body" >&2
              exit 1
            fi
          fi
          endpoints_changed=0
          openapi_changed=0
          if changed_file "docs/reference/contracts/schemas/ENDPOINTS.json"; then endpoints_changed=1; fi
          if changed_file "configs/openapi/v1/openapi.snapshot.json"; then openapi_changed=1; fi
          if [ "${endpoints_changed}" -ne "${openapi_changed}" ]; then
            echo "ENDPOINTS registry diff must match OpenAPI snapshot diff" >&2
            exit 1
          fi
          error_registry_changed=0
          error_runtime_changed=0
          if changed_file "docs/reference/contracts/schemas/ERROR_CODES.json"; then error_registry_changed=1; fi
          if changed_file "crates/bijux-atlas-api/src/generated/error_codes.rs" || changed_file "crates/bijux-atlas-core/src/generated/error_codes.rs" || changed_file "ops/inventory/meta/error-registry.json"; then error_runtime_changed=1; fi
          if [ "${error_registry_changed}" -ne "${error_runtime_changed}" ]; then
            echo "ERROR_CODES registry diff must match runtime error map diff" >&2
            exit 1
          fi
          config_registry_changed=0
          config_runtime_changed=0
          if changed_file "docs/reference/contracts/schemas/CONFIG_KEYS.json"; then config_registry_changed=1; fi
          if changed_file "crates/bijux-atlas-server/docs/generated/effective-config.snapshot.json" || changed_file "crates/bijux-atlas-server/docs/generated/runtime-startup-config.md" || changed_file "crates/bijux-atlas-server/docs/generated/runtime-startup-config.schema.json"; then config_runtime_changed=1; fi
          if [ "${config_registry_changed}" -ne "${config_runtime_changed}" ]; then
            echo "CONFIG_KEYS registry diff must match runtime effective config artifact diff" >&2
            exit 1
          fi
          # Generated reference drift mapping from SSOT surfaces.
          ssot_docs_pairs=0
          if changed_file "ops/inventory/root-surface.json"; then
            ssot_docs_pairs=1
            changed_file "docs/reference/repo-map.md" || { echo "root surface SSOT changed but docs/reference/repo-map.md was not updated" >&2; exit 1; }
          fi
          if changed_file "configs/ops/make-target-registry.json"; then
            ssot_docs_pairs=1
            changed_file "docs/_internal/generated/make-targets.md" || { echo "public target registry changed but docs/_internal/generated/make-targets.md was not updated" >&2; exit 1; }
          fi
          if changed_file "docs/_internal/registry/registry.json"; then
            ssot_docs_pairs=1
            changed_file "docs/_internal/governance/metadata/front-matter.index.json" || { echo "docs registry changed but front-matter index was not updated" >&2; exit 1; }
          fi
          if [ "${ssot_docs_pairs}" -eq 1 ]; then
            jq -n '{schema_version:1,kind:"docs_ssot_reference_drift_gate",status:"ok"}' > "${report_root}/docs-ssot-reference-drift.json"
          else
            jq -n '{schema_version:1,kind:"docs_ssot_reference_drift_gate",status:"not_applicable"}' > "${report_root}/docs-ssot-reference-drift.json"
          fi
          # Golden workflow docs must include executable command and expected output guidance.
          rg -n "^bijux dev atlas demo quickstart --format json$" docs/start-here.md > /dev/null
          rg -n "Expected output:" docs/start-here.md docs/control-plane/lane-matrix.md > /dev/null
          jq -n '{schema_version:1,kind:"docs_golden_workflow_verification",status:"ok"}' > "${report_root}/docs-golden-workflow-verification.json"
          # Docs external link allowlist requires expiry and owner for each entry.
          python - <<'PY'
          import json, pathlib
          allow = json.loads(pathlib.Path("configs/docs/external-link-allowlist.json").read_text())
          entries = allow.get("entries", [])
          errors = []
          for i, row in enumerate(entries):
              if not row.get("pattern"):
                  errors.append(f"entries[{i}] missing pattern")
              if not row.get("owner"):
                  errors.append(f"entries[{i}] missing owner")
              if not row.get("expires_on"):
                  errors.append(f"entries[{i}] missing expires_on")
          if errors:
              raise SystemExit("docs external-link allowlist contract failed: " + "; ".join(errors))
          pathlib.Path(__import__("os").environ["REPORT_ROOT"], "docs-link-allowlist-policy.json").write_text(json.dumps({
              "schema_version": 1,
              "kind": "docs_link_allowlist_policy",
              "status": "ok",
              "entries": len(entries)
          }, indent=2) + "\n")
          PY
          # Alert/runbook and SLO/load/e2e mapping contracts.
          python - <<'PY'
          import json, pathlib, re
          root = pathlib.Path(".")
          report_root = pathlib.Path(__import__("os").environ["REPORT_ROOT"])
          alerts = json.loads((root / "ops/observe/contracts/alerts-contract.json").read_text())["alert_specs"]
          alert_catalog = {r["id"] for r in json.loads((root / "ops/observe/alert-catalog.json").read_text())["alerts"]}
          runbook_dir = root / "docs/operations/runbooks"
          runbook_ids = {p.stem for p in runbook_dir.glob("*.md")}
          missing_alerts = sorted([k for k in alerts.keys() if k not in alert_catalog])
          missing_runbooks = sorted([k for k,v in alerts.items() if v.get("runbook_id","") not in runbook_ids])
          if missing_alerts or missing_runbooks:
              raise SystemExit(f"alerts/runbooks mapping failed missing_alerts={missing_alerts} missing_runbooks={missing_runbooks}")
          slo = json.loads((root / "configs/slo/slo.json").read_text())
          slis = {r["id"] for r in slo["slis"]}
          slos = slo["slos"]
          bad_slos = [row["id"] for row in slos if row.get("sli") not in slis]
          if bad_slos:
              raise SystemExit("slo mapping failed: unknown sli in " + ", ".join(sorted(bad_slos)))
          dashboards = list((root / "ops/observe/dashboards").glob("*.json"))
          if not dashboards:
              raise SystemExit("slo mapping failed: no dashboards found under ops/observe/dashboards")
          load_suites = json.loads((root / "ops/load/generated/suites.manifest.json").read_text())["suites"]
          capacity_doc = root / "docs/operations/load/suites.md"
          if not capacity_doc.exists():
              raise SystemExit("load suite mapping failed: docs/operations/load/suites.md missing")
          no_purpose = [row["name"] for row in load_suites if not row.get("purpose")]
          if no_purpose:
              raise SystemExit("load suites missing purpose: " + ", ".join(sorted(no_purpose)))
          e2e_scenarios = json.loads((root / "ops/e2e/scenarios/scenarios.json").read_text())["scenarios"]
          triage_doc = root / "docs/operations/runbooks/load-failure-triage.md"
          if not triage_doc.exists():
              raise SystemExit("e2e mapping failed: load-failure-triage runbook missing")
          bad_e2e = [row["id"] for row in e2e_scenarios if not row.get("entrypoint")]
          if bad_e2e:
              raise SystemExit("e2e scenarios missing entrypoint: " + ", ".join(sorted(bad_e2e)))
          (report_root / "ops-alert-runbook-slo-load-e2e-map.json").write_text(json.dumps({
              "schema_version": 1,
              "kind": "ops_alert_runbook_slo_load_e2e_map",
              "status": "ok",
              "alert_count": len(alerts),
              "runbook_count": len(runbook_ids),
              "slo_count": len(slos),
              "sli_count": len(slis),
              "load_suite_count": len(load_suites),
              "e2e_scenario_count": len(e2e_scenarios)
          }, indent=2) + "\n")
          PY
          # Registry contract groups must not be empty (domain and suite).
          python - <<'PY'
          import pathlib, tomllib, json
          report_root = pathlib.Path(__import__("os").environ["REPORT_ROOT"])
          reg = tomllib.loads(pathlib.Path("ops/inventory/registry.toml").read_text())
          checks = reg.get("checks", [])
          suites = reg.get("suites", [])
          domain_counts = {}
          for row in checks:
              d = row.get("domain","").strip()
              domain_counts[d] = domain_counts.get(d,0)+1
          empty_domains = sorted([d for d,c in domain_counts.items() if d and c == 0])
          empty_suite_refs = sorted([s.get("id","") for s in suites if not s.get("checks")])
          if empty_domains or empty_suite_refs:
              raise SystemExit(f"registry empty groups found domains={empty_domains} suites={empty_suite_refs}")
          (report_root / "registry-group-coverage.json").write_text(json.dumps({
              "schema_version": 1,
              "kind": "registry_group_coverage",
              "status": "ok",
              "domains": domain_counts,
              "suite_count": len(suites)
          }, indent=2) + "\n")
          PY
          # Make surface must derive from one source.
          jq -r '.public_targets[]' make/target-list.json | sort > "${report_root}/make-public-targets.txt"
          jq -r '.targets[] | select(.visibility=="public") | .name' configs/ops/make-target-registry.json | sort > "${report_root}/make-registry-public-targets.txt"
          if ! diff -u "${report_root}/make-registry-public-targets.txt" "${report_root}/make-public-targets.txt" > "${report_root}/make-surface-drift.diff"; then
            echo "public wrapper surface drift: make/target-list.json does not match configs/ops/make-target-registry.json" >&2
            exit 1
          fi
          jq -n --rawfile registry "${report_root}/make-registry-public-targets.txt" --rawfile surface "${report_root}/make-public-targets.txt" '{
            schema_version: 1,
            kind: "make_surface_drift",
            status: "ok",
            registry_public_targets: ($registry | split("\n") | map(select(length>0))),
            surfaced_public_targets: ($surface | split("\n") | map(select(length>0)))
          }' > "${report_root}/make-surface-drift.json"
          # Deterministic check run order report.
          python - <<'PY'
          import json, pathlib
          report_root = pathlib.Path(__import__("os").environ["REPORT_ROOT"])
          for suite in ["docs_required", "configs_required", "make_required", "repo_required", "ci_fast"]:
              path = report_root / f"suite-membership-{suite}.json"
              if not path.exists():
                  continue
              payload = json.loads(path.read_text())
              ids = [row["id"] for row in payload.get("checks", [])]
              if ids != sorted(ids):
                  raise SystemExit(f"suite membership order is not deterministic for {suite}")
          (report_root / "deterministic-check-order.json").write_text(json.dumps({
              "schema_version": 1,
              "kind": "deterministic_check_order",
              "status": "ok",
          }, indent=2) + "\n")
          PY
          # Check metadata report for purpose/failure/fix and stable machine error code.
          python - <<'PY'
          import json, pathlib, re, tomllib
          repo = pathlib.Path(".")
          report_root = pathlib.Path(__import__("os").environ["REPORT_ROOT"])
          reg = tomllib.loads((repo / "ops/inventory/registry.toml").read_text())
          checks = []
          for row in reg.get("checks", []):
              cid = row["id"].strip()
              docs = row["docs"].strip()
              title = row["title"].strip()
              if not cid or not title or not docs:
                  raise SystemExit(f"registry check metadata missing required fields for {cid or '<unknown>'}")
              checks.append({
                  "id": cid,
                  "purpose": title,
                  "failure_mode": f"{cid} emits violation records when invariant is broken",
                  "fix_hint": f"follow {docs}",
                  "error_code": cid.replace("-", "_").upper(),
              })
          checks.sort(key=lambda r: r["id"])
          (report_root / "check-metadata-catalog.json").write_text(json.dumps({
              "schema_version": 1,
              "kind": "check_metadata_catalog",
              "checks": checks,
          }, indent=2) + "\n")
          PY
          # Evidence artifacts must be schema-versioned where applicable.
          python - <<'PY'
          import json, pathlib
          report_root = pathlib.Path(__import__("os").environ["REPORT_ROOT"])
          missing = []
          for path in sorted(report_root.glob("*.json")):
              try:
                  payload = json.loads(path.read_text())
              except Exception:
                  continue
              if isinstance(payload, dict):
                  if "schema_version" not in payload:
                      missing.append(path.name)
          if missing:
              raise SystemExit("report JSON missing schema_version: " + ", ".join(missing))
          (report_root / "artifact-schema-version-check.json").write_text(json.dumps({
              "schema_version": 1,
              "kind": "artifact_schema_version_check",
              "status": "ok",
          }, indent=2) + "\n")
          PY
          # Artifact naming and size budget.
          python - <<'PY'
          import json, os, pathlib, re
          report_root = pathlib.Path(os.environ["REPORT_ROOT"])
          bad_names = []
          bad_sizes = []
          for path in report_root.rglob("*"):
              if not path.is_file():
                  continue
              rel = path.relative_to(report_root).as_posix()
              if not re.fullmatch(r"[a-z0-9][a-z0-9._/-]*", rel):
                  bad_names.append(rel)
              if path.stat().st_size > 5 * 1024 * 1024:
                  bad_sizes.append({"path": rel, "size": path.stat().st_size})
          if bad_names:
              raise SystemExit("artifact naming policy violation: " + ", ".join(bad_names))
          if bad_sizes:
              raise SystemExit("artifact size budget exceeded: " + ", ".join(f"{row['path']}={row['size']}" for row in bad_sizes))
          (report_root / "artifact-naming-and-size.json").write_text(json.dumps({
              "schema_version": 1,
              "kind": "artifact_naming_and_size",
              "status": "ok",
          }, indent=2) + "\n")
          PY
          cargo run -q -p bijux-dev-atlas -- contracts runtime --mode static --format json --artifacts-root "${contracts_root}" > "${contracts_root}/runtime-static.json" 2> "${logs_root}/runtime-contracts-static.stderr.log"
          cargo run -q -p bijux-dev-atlas -- contracts control-plane --mode static --format json --artifacts-root "${contracts_root}" > "${contracts_root}/control-plane-static.json" 2> "${logs_root}/control-plane-contracts-static.stderr.log"
          cargo run -q -p bijux-dev-atlas -- contracts configs --mode static --format json --artifacts-root "${contracts_root}" > "${contracts_root}/configs-static.json" 2> "${logs_root}/configs-contracts-static.stderr.log"
          cargo test --workspace --locked > "${logs_root}/workspace-test.stdout.log" 2> "${logs_root}/workspace-test.stderr.log"
          cargo test -q -p bijux-atlas-api openapi_snapshot_is_deterministic_and_matches_committed_contract -- --exact > "${logs_root}/openapi-snapshot.stdout.log" 2> "${logs_root}/openapi-snapshot.stderr.log"
          cargo test -q -p bijux-atlas-api openapi_hash_matches_pinned_contract -- --exact > "${logs_root}/openapi-hash.stdout.log" 2> "${logs_root}/openapi-hash.stderr.log"
          cargo test -q -p bijux-atlas-server runtime_startup_config_contract_artifacts_match_generated > "${logs_root}/runtime-startup-config-contract.stdout.log" 2> "${logs_root}/runtime-startup-config-contract.stderr.log"
          cargo run -q -p bijux-dev-atlas -- contracts make --mode static --format text --artifacts-root "${contracts_root}" > "${contracts_root}/make-static.txt" 2> "${logs_root}/make-contracts-static.stderr.log"
          cargo run -q -p bijux-dev-atlas -- contracts docker --mode static --format json --artifacts-root "${contracts_root}" > "${contracts_root}/docker-static.json" 2> "${logs_root}/docker-contracts-static.stderr.log"
          cargo run -q -p bijux-dev-atlas -- contracts ops --mode static --format json --artifacts-root "${contracts_root}" > "${contracts_root}/ops-static.json" 2> "${logs_root}/ops-contracts-static.stderr.log"
          cargo test -q -p bijux-dev-atlas --test ops_contract_registry_snapshot -- --nocapture > "${logs_root}/ops-contract-snapshot.stdout.log" 2> "${logs_root}/ops-contract-snapshot.stderr.log"
          for file in \
            ops/inventory/contract-gate-map.json \
            ops/inventory/surfaces.json \
            ops/schema/generated/schema-index.json \
            ops/_generated.example/contracts-registry-snapshot.json; do
            jq -S . "${file}" > "${file}.canon"
            mv "${file}.canon" "${file}"
          done
          git diff --exit-code -- \
            ops/inventory/contract-gate-map.json \
            ops/inventory/surfaces.json \
            ops/schema/generated/schema-index.json \
            ops/_generated.example/contracts-registry-snapshot.json > "${logs_root}/contracts-canonical-diff.log"
          jq -r '.tests[] | select(.status=="FAIL" or .status=="ERROR") | "\(.contract_id) \(.test_id) \(.status) \((.violations | map(.message) | join("; ")))"' \
            "${contracts_root}/ops-static.json" | while IFS= read -r row; do
            [ -z "${row}" ] && continue
            contract_id="$(echo "${row}" | awk '{print $1}')"
            test_id="$(echo "${row}" | awk '{print $2}')"
            message="$(echo "${row}" | cut -d' ' -f4-)"
            echo "::error title=OPS Contract ${contract_id}/${test_id}::${message}"
          done
          jq -r '.tests[] | select(.status=="FAIL" or .status=="ERROR") | "\(.contract_id) \(.test_id) \((.violations | map(.message) | join("; ")))"' \
            "${contracts_root}/ops-static.json" > "${contracts_root}/ops-failures.txt"
          jq -r '
            . as $r
            | $r.tests
            | group_by((.contract_id | split("-")[1]))[]
            | {
                pillar: (.[0].contract_id | split("-")[1]),
                contracts: (map(.contract_id) | unique | length),
                tests: length,
                pass: (map(select(.status=="PASS")) | length),
                fail: (map(select(.status=="FAIL")) | length),
                skip: (map(select(.status=="SKIP")) | length),
                error: (map(select(.status=="ERROR")) | length),
                pass_rate: ((map(select(.status=="PASS")) | length) / length)
              }
          ' "${contracts_root}/ops-static.json" > "${contracts_root}/ops-coverage-by-pillar.json"
          jq -n --argfile ops "${contracts_root}/ops-static.json" --argfile docker "${contracts_root}/docker-static.json" --argfile pillar "${contracts_root}/ops-coverage-by-pillar.json" '
            {
              schema_version: 1,
              kind: "contracts_coverage_v1",
              ops_summary: $ops.summary,
              docker_summary: $docker.summary,
              pillar_coverage: $pillar
            }
          ' > "${contracts_root}/coverage-summary.json"
          # Coverage heatmap + missing proof + proof debt ledger + proof-level guardrails.
          python - <<'PY'
          import json, pathlib, tomllib
          report_root = pathlib.Path(__import__("os").environ["REPORT_ROOT"])
          contracts_root = report_root / "contracts"
          reg = tomllib.loads(pathlib.Path("ops/inventory/registry.toml").read_text())
          checks = reg.get("checks", [])
          def classify(title: str) -> str:
              t = title.lower()
              shape_tokens = ["exists", "present", "path", "naming", "sorted", "format", "size", "budget", "allowlist"]
              return "shape" if any(tok in t for tok in shape_tokens) else "proof"
          rows = []
          by_domain = {}
          for row in checks:
              dom = row["domain"]
              kind = classify(row["title"])
              rows.append({"id": row["id"], "domain": dom, "title": row["title"], "classification": kind})
              by_domain.setdefault(dom, {"shape": 0, "proof": 0})
              by_domain[dom][kind] += 1
          heatmap = {
              "schema_version": 1,
              "kind": "contract_coverage_heatmap",
              "domains": by_domain
          }
          (report_root / "contract-coverage-heatmap.json").write_text(json.dumps(heatmap, indent=2) + "\n")
          missing_proof = [r for r in rows if r["classification"] == "shape"]
          (report_root / "missing-proof-report.json").write_text(json.dumps({
              "schema_version": 1,
              "kind": "missing_proof_report",
              "shape_only_checks": missing_proof
          }, indent=2) + "\n")
          (report_root / "proof-debt-ledger.json").write_text(json.dumps({
              "schema_version": 1,
              "kind": "proof_debt_ledger",
              "count": len(missing_proof),
              "items": missing_proof
          }, indent=2) + "\n")
          policy = json.loads(pathlib.Path("ops/policy/proof-level-checks.json").read_text())
          required = set(policy.get("proof_level_check_ids", []))
          known = {r["id"] for r in rows}
          missing = sorted(required - known)
          if len(required) < 10 or missing:
              raise SystemExit(f"proof-level checks policy invalid len={len(required)} missing={missing}")
          (report_root / "proof-level-policy-check.json").write_text(json.dumps({
              "schema_version": 1,
              "kind": "proof_level_policy_check",
              "status": "ok",
              "required_count": len(required)
          }, indent=2) + "\n")
          PY
          # Flake detector: rerun root static contracts and compare normalized result.
          cargo run -q -p bijux-dev-atlas -- contracts root --mode static --format json --artifacts-root "${contracts_root}" > "${contracts_root}/root-static-first.json" 2> "${logs_root}/root-static-first.stderr.log"
          cargo run -q -p bijux-dev-atlas -- contracts root --mode static --format json --artifacts-root "${contracts_root}" > "${contracts_root}/root-static-rerun.json" 2> "${logs_root}/root-static-rerun.stderr.log"
          jq 'del(.summary.duration_ms) | .tests |= map(del(.duration_ms))' "${contracts_root}/root-static-first.json" > "${contracts_root}/root-static-first.normalized.json"
          jq 'del(.summary.duration_ms) | .tests |= map(del(.duration_ms))' "${contracts_root}/root-static-rerun.json" > "${contracts_root}/root-static-rerun.normalized.json"
          if ! diff -u "${contracts_root}/root-static-first.normalized.json" "${contracts_root}/root-static-rerun.normalized.json" > "${report_root}/contract-flake-detector.diff"; then
            echo "contract flake detector failed: root static rerun produced different normalized results" >&2
            exit 1
          fi
          jq -n '{schema_version:1,kind:"contract_flake_detector",status:"ok"}' > "${report_root}/contract-flake-detector.json"
          # Contract performance budgets with expiring waivers.
          python - <<'PY'
          import json, pathlib, datetime
          root = pathlib.Path(".")
          report_root = pathlib.Path(__import__("os").environ["REPORT_ROOT"])
          contracts_root = report_root / "contracts"
          policy = json.loads((root / "ops/policy/contracts-performance-budget.json").read_text())
          budgets = policy.get("budgets_ms", {})
          waivers = policy.get("waivers", [])
          today = datetime.date.fromisoformat("2026-03-01")
          active_waivers = {}
          for row in waivers:
              exp = datetime.date.fromisoformat(row["expires_on"])
              if exp < today:
                  raise SystemExit(f"expired contract performance waiver: {row}")
              active_waivers[row["domain"]] = row
          files = {
              "runtime": contracts_root / "runtime-static.json",
              "control_plane": contracts_root / "control-plane-static.json",
              "configs": contracts_root / "configs-static.json",
              "docker": contracts_root / "docker-static.json",
              "ops": contracts_root / "ops-static.json",
              "root": contracts_root / "root-static-first.json",
          }
          measured = {}
          over = []
          for domain, path in files.items():
              payload = json.loads(path.read_text())
              dur = int(payload.get("summary", {}).get("duration_ms", 0))
              measured[domain] = dur
              budget = int(budgets.get(domain, 0))
              if budget and dur > budget and domain not in active_waivers:
                  over.append((domain, dur, budget))
          if over:
              raise SystemExit("contract performance budget exceeded: " + "; ".join(f"{d} {m}>{b}" for d,m,b in over))
          (report_root / "contract-performance-budget.json").write_text(json.dumps({
              "schema_version": 1,
              "kind": "contract_performance_budget",
              "status": "ok",
              "measured_ms": measured,
              "budgets_ms": budgets,
              "active_waivers": active_waivers
          }, indent=2) + "\n")
          PY
          make ops-fast > "${report_root}/ci-fast.json" 2> "${logs_root}/ci-fast.stderr.log"
          cargo run -q -p bijux-dev-atlas -- check list --suite ci_fast --include-internal --include-slow --format json > "${report_root}/suite-membership-ci_fast.json"
          cargo run -q -p bijux-dev-atlas -- demo quickstart --format json > "${report_root}/demo-quickstart.json"
          python - <<'PY'
          import json, os, pathlib
          run_id=os.environ["RUN_ID"]
          p=pathlib.Path(f"artifacts/{run_id}/reports/demo-quickstart.json")
          payload=json.loads(p.read_text())
          assert payload["schema_version"]==1
          assert payload["steps_budget"] <= 5
          assert payload["duration_budget_minutes"] <= 3
          assert len(payload["steps"]) <= payload["steps_budget"]
          PY
          # Docs entrypoints/nav/metadata/directory-budget contracts.
          python - <<'PY'
          import json, pathlib, yaml, os
          root = pathlib.Path(".")
          report_root = pathlib.Path(os.environ["REPORT_ROOT"])
          mkdocs = yaml.safe_load((root / "mkdocs.yml").read_text())
          nav = mkdocs.get("nav", [])
          top_names = []
          def top_label(item):
              if isinstance(item, dict):
                  return next(iter(item.keys()))
              return str(item)
          top_names = [top_label(x) for x in nav]
          if top_names[:2] != ["Home", "Start Here"]:
              raise SystemExit(f"docs entrypoint contract failed: first two nav items must be Home/Start Here, got {top_names[:2]}")
          nav_text = (root / "mkdocs.yml").read_text()
          if "_internal/" in nav_text:
              raise SystemExit("published nav must not include docs/_internal pages")
          registry = json.loads((root / "docs/_internal/registry/registry.json").read_text())
          missing_meta = []
          for row in registry.get("documents", []):
              path = row.get("path", "")
              if not path.startswith("docs/") or path.startswith("docs/_internal/"):
                  continue
              required = ["audience", "title", "stability", "owner", "last_reviewed"]
              for key in required:
                  if not row.get(key):
                      missing_meta.append(f"{path}:{key}")
          if missing_meta:
              raise SystemExit("published docs metadata missing required fields: " + ", ".join(missing_meta[:20]))
          limits = json.loads((root / "docs/_internal/governance/metadata/directory-budget-exceptions.json").read_text())
          max_files = int(limits["max_files"])
          max_subdirs = int(limits["max_subdirs"])
          exceptions = {row["path"]: row for row in limits.get("exceptions", [])}
          violations = []
          for d in sorted((root / "docs").rglob("*")):
              if not d.is_dir():
                  continue
              rel = d.as_posix()
              if "/_internal/generated" in rel:
                  continue
              files = [p for p in d.iterdir() if p.is_file() and not p.name.startswith(".")]
              dirs = [p for p in d.iterdir() if p.is_dir() and not p.name.startswith(".")]
              if len(files) <= max_files and len(dirs) <= max_subdirs:
                  continue
              if rel in exceptions:
                  continue
              violations.append({"path": rel, "files": len(files), "subdirs": len(dirs)})
          if violations:
              raise SystemExit("docs directory budget exceeded without exception proof: " + json.dumps(violations[:20]))
          (report_root / "docs-entrypoint-and-budget-contracts.json").write_text(json.dumps({
              "schema_version": 1,
              "kind": "docs_entrypoint_and_budget_contracts",
              "status": "ok",
              "top_nav": top_names,
              "published_docs_count": len([r for r in registry.get("documents", []) if str(r.get("path","")).startswith("docs/") and not str(r.get("path","")).startswith("docs/_internal/")]),
              "directory_exception_count": len(exceptions)
          }, indent=2) + "\n")
          PY
          python - <<'PY'
          import json, os, pathlib, re
          root = pathlib.Path(".")
          report_root = pathlib.Path(os.environ["REPORT_ROOT"])
          docs_root = root / "docs"
          spine_pages = [
              "docs/index.md",
              "docs/start-here.md",
              "docs/product/what-is-bijux-atlas.md",
              "docs/architecture/index.md",
              "docs/operations/index.md",
              "docs/control-plane/index.md",
              "docs/api/index.md",
              "docs/development/index.md",
              "docs/reference/index.md",
          ]
          required_glossary_terms = [
              "Atlas",
              "Contract",
              "Reader spine",
              "Runbook",
              "Reference",
              "Control plane",
              "Reproducibility",
              "Release",
              "Dataset",
          ]
          # 1) Every published docs directory must expose index.md.
          index_violations = []
          for d in sorted(docs_root.rglob("*")):
              if not d.is_dir():
                  continue
              rel = d.as_posix()
              if rel.startswith("docs/_internal/") or rel.startswith("docs/_assets/") or rel.startswith("docs/_drafts/"):
                  continue
              entries = [p for p in d.iterdir() if not p.name.startswith(".")]
              if not entries:
                  continue
              if not (d / "index.md").exists():
                  index_violations.append(rel)
          if index_violations:
              raise SystemExit("docs directory index contract failed; missing index.md in: " + ", ".join(index_violations[:20]))
          # 2) Home page keeps exactly three curated reader tracks.
          home = (root / "docs/index.md").read_text()
          m = re.search(r"## Choose your path\\n\\n(?P<body>.*?)(\\n## |\\Z)", home, flags=re.S)
          if not m:
              raise SystemExit("docs/index.md must include a `## Choose your path` section")
          track_lines = [ln.strip()[2:].strip() for ln in m.group("body").splitlines() if ln.strip().startswith("- ")]
          expected_tracks = [
              "API user: begin with API",
              "Operator: begin with Operations",
              "Contributor: begin with Development",
          ]
          if track_lines != expected_tracks:
              raise SystemExit(f"docs/index.md choose-your-path contract failed: expected {expected_tracks}, got {track_lines}")
          # 3) Architecture / Operations / Control-plane must reference the product narrative spine.
          expected_spine_link = "../product/what-is-bijux-atlas.md"
          spine_link_violations = []
          for rel in ("docs/architecture/index.md", "docs/operations/index.md", "docs/control-plane/index.md"):
              txt = (root / rel).read_text()
              if expected_spine_link not in txt:
                  spine_link_violations.append(rel)
          if spine_link_violations:
              raise SystemExit("spine reference contract failed; missing product narrative link in: " + ", ".join(spine_link_violations))
          # 4) Glossary must define required terms and spine pages must link glossary.
          glossary_text = (root / "docs/glossary.md").read_text()
          defined = set(re.findall(r"^- `([^`]+)`:", glossary_text, flags=re.M))
          missing_terms = [t for t in required_glossary_terms if t not in defined]
          if missing_terms:
              raise SystemExit("glossary contract failed; missing required terms: " + ", ".join(missing_terms))
          glossary_link_violations = []
          for rel in spine_pages:
              txt = (root / rel).read_text()
              if "glossary.md" not in txt:
                  glossary_link_violations.append(rel)
          if glossary_link_violations:
              raise SystemExit("spine glossary-link contract failed: " + ", ".join(glossary_link_violations))
          # 5) Orphan report with where-to-link hints.
          all_docs = []
          for path in docs_root.rglob("*.md"):
              rel = path.as_posix()
              if rel.startswith("docs/_internal/") or rel.startswith("docs/_assets/") or rel.startswith("docs/_drafts/"):
                  continue
              all_docs.append(rel)
          all_set = set(all_docs)
          links = {rel: set() for rel in all_docs}
          md_link_re = re.compile(r"\\[[^\\]]+\\]\\(([^)]+)\\)")
          for rel in all_docs:
              src_path = root / rel
              text = src_path.read_text()
              for target in md_link_re.findall(text):
                  target = target.split("#")[0].strip()
                  if not target or target.startswith("http://") or target.startswith("https://") or target.startswith("mailto:"):
                      continue
                  cand = (src_path.parent / target).resolve()
                  try:
                      rel_target = cand.relative_to(root).as_posix()
                  except Exception:
                      continue
                  if rel_target in all_set:
                      links[rel].add(rel_target)
          reachable = set()
          queue = ["docs/index.md", "docs/start-here.md"]
          while queue:
              cur = queue.pop(0)
              if cur in reachable:
                  continue
              reachable.add(cur)
              for nxt in sorted(links.get(cur, [])):
                  if nxt not in reachable:
                      queue.append(nxt)
          orphans = sorted([p for p in all_docs if p not in reachable])
          orphan_hints = []
          for orphan in orphans:
              parent = pathlib.Path(orphan).parent.as_posix()
              parent_index = f"{parent}/index.md" if parent != "docs" else "docs/index.md"
              if parent_index == orphan:
                  parent_index = "docs/index.md"
              orphan_hints.append({
                  "path": orphan,
                  "suggested_link_source": parent_index,
                  "reason": "orphan page is unreachable from docs/index.md or docs/start-here.md",
              })
          # 6) Duplicate topic collisions (top 20 by Jaccard similarity).
          tokens = {}
          for rel in all_docs:
              text = (root / rel).read_text()
              words = re.findall(r"[A-Za-z0-9]{5,}", text.lower())
              tokens[rel] = set(words)
          collisions = []
          docs_list = sorted(all_docs)
          for i, left in enumerate(docs_list):
              for right in docs_list[i + 1:]:
                  lt = tokens[left]
                  rt = tokens[right]
                  if not lt or not rt:
                      continue
                  inter = len(lt & rt)
                  if inter == 0:
                      continue
                  union = len(lt | rt)
                  score = inter / union
                  if score >= 0.35:
                      collisions.append({"left": left, "right": right, "similarity": round(score, 3), "shared_token_count": inter})
          collisions.sort(key=lambda row: (-row["similarity"], row["left"], row["right"]))
          top_collisions = collisions[:20]
          payload = {
              "schema_version": 1,
              "kind": "docs_navigation_and_semantics_contracts",
              "status": "ok",
              "directory_index_missing": [],
              "orphan_hints": orphan_hints,
              "duplicate_topic_collisions_top20": top_collisions,
              "three_reader_tracks": expected_tracks,
              "spine_pages_checked": spine_pages,
              "required_glossary_terms": required_glossary_terms,
          }
          (report_root / "docs-navigation-and-semantics-contracts.json").write_text(json.dumps(payload, indent=2) + "\\n")
          PY
          python - <<'PY'
          import datetime, json, os, pathlib, re, yaml
          root = pathlib.Path(".")
          report_root = pathlib.Path(os.environ["REPORT_ROOT"])
          docs_root = root / "docs"
          today = datetime.date.fromisoformat("2026-03-01")
          spine_pages = [
              "docs/index.md",
              "docs/start-here.md",
              "docs/product/what-is-bijux-atlas.md",
              "docs/architecture/index.md",
              "docs/operations/index.md",
              "docs/control-plane/index.md",
              "docs/api/index.md",
              "docs/development/index.md",
              "docs/reference/index.md",
          ]
          # Docs governance sources of truth.
          registry = json.loads((root / "docs/_internal/registry/registry.json").read_text())
          quality_policy = json.loads((root / "configs/docs/quality-policy.json").read_text())
          stale_days = int(quality_policy.get("stale_days", 90))
          external_allow = json.loads((root / "configs/docs/external-link-allowlist.json").read_text())
          synonyms = json.loads((root / "docs/_internal/governance/metadata/search-synonyms.json").read_text())
          taxonomy = json.loads((root / "docs/_internal/governance/metadata/tag-taxonomy.json").read_text())
          nav_baseline_path = root / "docs/_internal/governance/metadata/nav-order.json"
          nav_baseline = json.loads(nav_baseline_path.read_text())
          mkdocs = yaml.safe_load((root / "mkdocs.yml").read_text())
          top_nav = []
          for row in mkdocs.get("nav", []):
              if isinstance(row, dict):
                  label, target = next(iter(row.items()))
                  top_nav.append({"label": label, "target": target if isinstance(target, str) else None})
          expected_nav = nav_baseline.get("top_nav", [])
          if top_nav != expected_nav:
              raise SystemExit("docs navigation drift detected; update docs/_internal/governance/metadata/nav-order.json with intentional migration evidence")
          if synonyms.get("schema_version") != "v1" or not synonyms.get("synonyms"):
              raise SystemExit("docs search synonyms SSOT invalid: docs/_internal/governance/metadata/search-synonyms.json")
          if taxonomy.get("schema_version") != "v1" or not taxonomy.get("tags"):
              raise SystemExit("docs taxonomy SSOT invalid: docs/_internal/governance/metadata/tag-taxonomy.json")
          # Review freshness SLA on stable published pages.
          stale_rows = []
          stable_paths = set()
          for row in registry.get("documents", []):
              path = str(row.get("path", ""))
              if not path.startswith("docs/") or path.startswith("docs/_internal/"):
                  continue
              if row.get("stability") != "stable":
                  continue
              stable_paths.add(path)
              reviewed = str(row.get("last_reviewed", "")).strip()
              try:
                  reviewed_date = datetime.date.fromisoformat(reviewed)
              except ValueError:
                  stale_rows.append({"path": path, "reason": f"invalid last_reviewed `{reviewed}`"})
                  continue
              age_days = (today - reviewed_date).days
              if age_days > stale_days:
                  stale_rows.append({"path": path, "age_days": age_days, "stale_days": stale_days})
          if stale_rows:
              raise SystemExit("docs review SLA failed for stable pages: " + json.dumps(stale_rows[:20]))
          # Reader-spine quality: readability, examples, no internal path leaks.
          readability_budget = 240
          spine_quality_violations = []
          ext_re = re.compile(r"\[[^\]]+\]\((https?://[^)]+)\)")
          md_link_re = re.compile(r"\[[^\]]+\]\(([^)]+)\)")
          allow_rows = []
          for entry in external_allow.get("entries", []):
              exp = datetime.date.fromisoformat(entry["expires_on"])
              if exp < today:
                  raise SystemExit(f"expired external-link allowlist entry: {entry}")
              allow_rows.append(entry["pattern"])
          for rel in spine_pages:
              text = (root / rel).read_text()
              line_count = len(text.splitlines())
              if line_count > readability_budget:
                  spine_quality_violations.append(f"{rel}: readability budget exceeded ({line_count}>{readability_budget})")
              has_example = ("## Example" in text) or ("```bash" in text) or ("```text" in text)
              if not has_example:
                  spine_quality_violations.append(f"{rel}: missing concrete example section or runnable code block")
              for forbidden in ("docs/_internal/", "docs/_generated/", "ops/_generated/", "ops/_generated.example/"):
                  if forbidden in text:
                      spine_quality_violations.append(f"{rel}: reader page must not reference internal tooling path `{forbidden}`")
              for url in ext_re.findall(text):
                  if not any(url.startswith(pattern) for pattern in allow_rows):
                      spine_quality_violations.append(f"{rel}: external URL not allowlisted `{url}`")
          if spine_quality_violations:
              raise SystemExit("spine quality contracts failed: " + "; ".join(spine_quality_violations[:20]))
          # Diagram textual explanation contract.
          diagram_violations = []
          mermaid_re = re.compile(r"```mermaid")
          image_re = re.compile(r"!\[[^\]]*\]\(([^)]+)\)")
          for row in registry.get("documents", []):
              rel = str(row.get("path", ""))
              if rel.startswith("docs/_internal/") or not rel.endswith(".md"):
                  continue
              text = (root / rel).read_text()
              has_diagram = bool(mermaid_re.search(text)) or bool(image_re.search(text))
              if not has_diagram:
                  continue
              if "## Text explanation" not in text and "## How to read this graph" not in text:
                  diagram_violations.append(rel)
          if diagram_violations:
              raise SystemExit("diagram contract failed; missing textual explanation section: " + ", ".join(diagram_violations[:20]))
          # No TODO markers in stable published docs.
          todo_violations = []
          for rel in sorted(stable_paths):
              text = (root / rel).read_text().lower()
              if "todo" in text:
                  todo_violations.append(rel)
          if todo_violations:
              raise SystemExit("stable docs must not contain TODO markers: " + ", ".join(todo_violations[:20]))
          # Zero broken links for published pages.
          all_docs = set()
          for path in docs_root.rglob("*.md"):
              rel = path.as_posix()
              if rel.startswith("docs/_internal/") or rel.startswith("docs/_assets/") or rel.startswith("docs/_drafts/"):
                  continue
              all_docs.add(rel)
          generated_ref_violations = []
          for rel in sorted(all_docs):
              text = (root / rel).read_text()
              if "docs/_generated/" in text:
                  generated_ref_violations.append(rel)
          if generated_ref_violations:
              raise SystemExit("reader docs must not reference docs/_generated paths: " + ", ".join(generated_ref_violations[:20]))
          broken = []
          for rel in sorted(all_docs):
              src = root / rel
              text = src.read_text()
              for raw in md_link_re.findall(text):
                  target = raw.split("#")[0].strip()
                  if not target or target.startswith("http://") or target.startswith("https://") or target.startswith("mailto:"):
                      continue
                  resolved = (src.parent / target).resolve()
                  try:
                      rel_target = resolved.relative_to(root).as_posix()
                  except ValueError:
                      broken.append({"source": rel, "target": raw, "resolved": str(resolved)})
                      continue
                  if rel_target not in all_docs and not resolved.exists():
                      broken.append({"source": rel, "target": raw, "resolved": rel_target})
          if broken:
              raise SystemExit("broken link zero goal failed for published docs: " + json.dumps(broken[:20]))
          cleanup_doc = (root / "docs/_internal/governance/docs-quarterly-cleanup.md").read_text()
          if "Run once per quarter." not in cleanup_doc or "docs shrink report" not in cleanup_doc:
              raise SystemExit("docs quarterly prune gate contract failed: docs-quarterly-cleanup.md missing cadence or budget evidence requirements")
          payload = {
              "schema_version": 1,
              "kind": "docs_quality_contracts",
              "status": "ok",
              "stale_days": stale_days,
              "spine_readability_budget": readability_budget,
              "stable_pages_checked": len(stable_paths),
              "search_synonyms_terms": len(synonyms.get("synonyms", [])),
              "taxonomy_tags": len(taxonomy.get("tags", [])),
              "top_nav": top_nav,
          }
          (report_root / "docs-quality-contracts.json").write_text(json.dumps(payload, indent=2) + "\n")
          PY
          python - <<'PY'
          import datetime, fnmatch, json, os, pathlib, re
          root = pathlib.Path(".")
          report_root = pathlib.Path(os.environ["REPORT_ROOT"])
          configs_root = root / "configs"
          today = datetime.date.fromisoformat("2026-03-01")
          groups = json.loads((root / "configs/inventory/groups.json").read_text())
          index_payload = json.loads((root / "configs/_generated/configs-index.json").read_text())
          owners_registry = json.loads((root / "configs/owners-registry.json").read_text())
          consumers_registry = json.loads((root / "configs/consumers-registry.json").read_text())
          schema_map = json.loads((root / "configs/schema-map.json").read_text())
          no_schema = json.loads((root / "configs/inventory/no-schema-justifications.json").read_text())
          config_keys_registry = json.loads((root / "docs/reference/contracts/schemas/CONFIG_KEYS.json").read_text())
          rename_policy = json.loads((root / "configs/policy/config-key-renames.json").read_text())
          allowed_groups = set(groups.get("allowed_groups", []))
          root_file_allow = {
              "CONSUMERS.json",
              "CONTRACT.md",
              "INDEX.md",
              "OWNERS.json",
              "README.md",
              "SCHEMAS.json",
              "configs.contracts.json",
              "consumers-registry.json",
              "inventory.json",
              "inventory/no-schema-justifications.json",
              "owners-registry.json",
              "schema-map.json",
          }
          exclusion_prefixes = [
              "configs/docs/.vale/styles/",
              "configs/ops/perf/baselines/",
          ]
          max_depth = 5
          max_files_per_dir = 40
          violations = []
          warnings = []
          file_depth = {}
          tracked_files = []
          structured_files = []
          for entry in no_schema.get("entries", []):
              exp = datetime.date.fromisoformat(entry["expires_on"])
              if exp < today:
                  violations.append(f"expired no-schema justification entry: {entry}")

          def has_no_schema_justification(path: str) -> bool:
              for entry in no_schema.get("entries", []):
                  if fnmatch.fnmatch(path, entry["pattern"]):
                      return True
              return False

          def is_structured(path: str) -> bool:
              return pathlib.Path(path).suffix in {".json", ".jsonc", ".toml", ".yaml", ".yml"}

          for path in sorted(configs_root.rglob("*")):
              rel = path.as_posix()
              if path.is_file():
                  if any(rel.startswith(prefix) for prefix in exclusion_prefixes):
                      continue
                  parts = path.relative_to(configs_root).parts
                  file_depth[rel] = len(parts)
                  tracked_files.append(rel)
                  if is_structured(rel):
                      structured_files.append(rel)
                  if len(parts) == 1 and path.name not in root_file_allow:
                      violations.append(f"unexpected root config file: {rel}")
                  if len(parts) > max_depth:
                      violations.append(f"config path exceeds depth budget {max_depth}: {rel}")
                  if len(parts) >= 2 and parts[0] not in allowed_groups and parts[0] != "_generated":
                      violations.append(f"config top-level group is not allowed: {rel}")
                  # Directory and file naming normalization.
                  for segment in parts[:-1]:
                      if segment.startswith("."):
                          continue
                      if not re.fullmatch(r"[a-z0-9][a-z0-9_-]*", segment):
                          violations.append(f"config directory naming must be normalized: {rel}")
                          break
                  filename = parts[-1]
                  if filename.startswith("."):
                      pass
                  elif not re.fullmatch(r"[a-z0-9][a-z0-9._-]*", filename):
                      violations.append(f"config filename naming must be normalized: {rel}")
          for directory in sorted(configs_root.rglob("*")):
              if not directory.is_dir():
                  continue
              rel = directory.as_posix()
              files = [p for p in directory.iterdir() if p.is_file() and not p.name.startswith(".")]
              if len(files) > max_files_per_dir:
                  violations.append(f"config directory exceeds file budget {max_files_per_dir}: {rel} ({len(files)})")
          tail_to_groups = {}
          for path in file_depth:
              rel = pathlib.Path(path).relative_to("configs")
              parts = rel.parts
              if len(parts) < 3:
                  continue
              top = parts[0]
              tail = "/".join(parts[1:])
              tail_to_groups.setdefault(tail, set()).add(top)
          duplicated_tails = [{"tail": tail, "groups": sorted(groups)} for tail, groups in sorted(tail_to_groups.items()) if len(groups) > 1]
          if duplicated_tails:
              violations.append("duplicated config tree tails across groups detected")
          group_stability = {row["name"]: row.get("stability", "") for row in index_payload.get("groups", [])}
          allowed_lifecycle = {"stable", "experimental", "deprecated"}
          lifecycle_missing = []
          for path in file_depth:
              rel = pathlib.Path(path).relative_to("configs")
              if rel.parts[0] == "_generated":
                  continue
              group = rel.parts[0] if len(rel.parts) > 1 else None
              if group in allowed_groups:
                  lifecycle = group_stability.get(group, "")
                  if lifecycle not in allowed_lifecycle:
                      lifecycle_missing.append(path)
          if lifecycle_missing:
              violations.append("config lifecycle metadata missing or invalid for: " + ", ".join(lifecycle_missing[:20]))

          # Registry sorting normalization.
          groups_doc = json.loads((root / "configs/inventory/configs.json").read_text())
          group_names = [row.get("name", "") for row in groups_doc.get("groups", [])]
          if group_names != sorted(group_names):
              violations.append("configs/inventory/configs.json groups must be sorted by name")
          for row in groups_doc.get("groups", []):
              for key in ("public_files", "internal_files", "generated_files", "schemas", "tool_entrypoints"):
                  vals = row.get(key, [])
                  if isinstance(vals, list) and vals != sorted(vals):
                      violations.append(f"configs/inventory/configs.json group `{row.get('name')}` key `{key}` must be sorted")

          # Owner / consumer / schema coverage with explicit no-schema justifications.
          owner_patterns = list((owners_registry.get("files") or {}).keys())
          consumer_patterns = list((consumers_registry.get("files") or {}).keys())
          schema_patterns = list((schema_map.get("files") or {}).keys())
          schema_covered_structured = []
          missing_owner = []
          missing_consumer = []
          missing_schema_or_justification = []
          for rel in sorted(tracked_files):
              if rel.startswith("configs/_generated/"):
                  continue
              group_name = pathlib.Path(rel).parts[1] if len(pathlib.Path(rel).parts) > 1 else None
              has_owner = any(fnmatch.fnmatch(rel, pat) for pat in owner_patterns) or (group_name in (owners_registry.get("groups") or {}))
              if not has_owner:
                  missing_owner.append(rel)
              has_consumer = any(fnmatch.fnmatch(rel, pat) for pat in consumer_patterns) or (group_name in (consumers_registry.get("groups") or {}))
              if not has_consumer:
                  missing_consumer.append(rel)
              has_schema = any(fnmatch.fnmatch(rel, pat) for pat in schema_patterns)
              if is_structured(rel):
                  if has_schema:
                      schema_covered_structured.append(rel)
                  elif not has_no_schema_justification(rel):
                      missing_schema_or_justification.append(rel)
              elif not has_schema and not has_no_schema_justification(rel):
                  missing_schema_or_justification.append(rel)
          if missing_owner:
              violations.append("config owner coverage missing: " + ", ".join(missing_owner[:20]))
          if missing_consumer:
              violations.append("config consumer coverage missing: " + ", ".join(missing_consumer[:20]))
          if missing_schema_or_justification:
              violations.append("config schema/no-schema justification coverage missing: " + ", ".join(missing_schema_or_justification[:20]))

          # Schema usage and stable-schema example coverage.
          referenced_schema_targets = set((schema_map.get("files") or {}).values())
          all_schemas = set()
          for p in (root / "configs/schema").rglob("*.json"):
              rel = p.as_posix()
              if rel.startswith("configs/schema/generated/"):
                  continue
              all_schemas.add(rel)
          unused_schemas = sorted(all_schemas - referenced_schema_targets)
          if unused_schemas:
              violations.append("unused schema files detected: " + ", ".join(unused_schemas[:20]))
          missing_schema_examples = []
          for schema_path in sorted(referenced_schema_targets):
              # require at least one mapped file for schema path
              has_mapped = any(target == schema_path for target in (schema_map.get("files") or {}).values())
              if not has_mapped:
                  missing_schema_examples.append(schema_path)
          if missing_schema_examples:
              violations.append("schema without mapped examples/config files: " + ", ".join(missing_schema_examples[:20]))

          # Config key registry SSOT / usage / hidden-default checks.
          registry_keys = sorted(config_keys_registry.get("env_keys", []))
          key_usage = []
          all_code_files = []
          for code_root in ("crates", "ops", "configs"):
              base = root / code_root
              if not base.exists():
                  continue
              for p in base.rglob("*"):
                  if not p.is_file():
                      continue
                  if p.suffix not in {".rs", ".toml", ".json", ".yaml", ".yml", ".md", ".txt"}:
                      continue
                  all_code_files.append(p)
          code_text_cache = {}
          for p in all_code_files:
              try:
                  code_text_cache[p.as_posix()] = p.read_text()
              except Exception:
                  code_text_cache[p.as_posix()] = ""
          for key in registry_keys:
              hit_count = 0
              sample_paths = []
              for path, text in code_text_cache.items():
                  if key in text:
                      hit_count += 1
                      if len(sample_paths) < 5:
                          sample_paths.append(path)
              key_usage.append({"key": key, "hits": hit_count, "sample_paths": sample_paths})
          # enforce runtime ATLAS_* keys have at least one code reference
          missing_key_usage = [row["key"] for row in key_usage if row["key"].startswith("ATLAS_") and row["hits"] == 0]
          if missing_key_usage:
              violations.append("config keys missing code usage mapping: " + ", ".join(missing_key_usage[:20]))
          # hidden defaults / undeclared keys
          declared = set(registry_keys)
          undeclared_keys = set()
          for path, text in code_text_cache.items():
              for match in re.findall(r"ATLAS_[A-Z0-9_]+", text):
                  if match not in declared:
                      undeclared_keys.add(match)
          if undeclared_keys:
              violations.append("undeclared ATLAS_ keys found outside config key registry: " + ", ".join(sorted(undeclared_keys)[:20]))

          # Rename compatibility policy.
          if rename_policy.get("schema_version") != 1:
              violations.append("configs/policy/config-key-renames.json schema_version must be 1")
          rename_violations = []
          for row in rename_policy.get("renames", []):
              old_key = row.get("old_key", "")
              new_key = row.get("new_key", "")
              if not old_key.startswith("ATLAS_") or not new_key.startswith("ATLAS_"):
                  rename_violations.append(f"invalid rename row {row}")
                  continue
              if new_key not in declared:
                  rename_violations.append(f"rename target not present in config key registry: {new_key}")
          if rename_violations:
              violations.append("config key rename policy violations: " + "; ".join(rename_violations[:20]))

          # Secrets scan on stable configs: zero tolerance.
          secret_hits = []
          secret_patterns = [
              r"AKIA[0-9A-Z]{16}",
              r"-----BEGIN (?:RSA |EC |OPENSSH )?PRIVATE KEY-----",
              r"(?i)(password|secret|token)\\s*[:=]\\s*[\"'][^\"']+[\"']",
          ]
          for rel in sorted(structured_files):
              if rel.startswith("configs/_generated/"):
                  continue
              txt = (root / rel).read_text()
              for pat in secret_patterns:
                  if re.search(pat, txt):
                      secret_hits.append({"path": rel, "pattern": pat})
                      break
          if secret_hits:
              violations.append("secrets scan failed for stable configs: " + json.dumps(secret_hits[:20]))

          # Change checklist quality gate.
          checklist = (root / "docs/development/config-change-checklist.md").read_text()
          checklist_requirements = [
              "registry",
              "docs",
              "examples",
          ]
          for token in checklist_requirements:
              if token not in checklist.lower():
                  violations.append(f"config change checklist missing required section token `{token}`")

          # Semantic diff / debt / graph artifacts.
          rows = []
          for rel in sorted(tracked_files):
              data = (root / rel).read_bytes()
              rows.append({"path": rel, "sha256": __import__("hashlib").sha256(data).hexdigest(), "bytes": len(data)})
          semantic_diff_payload = {
              "schema_version": 1,
              "kind": "configs_semantic_diff",
              "status": "ok",
              "rows": rows,
          }
          (report_root / "configs-semantic-diff.json").write_text(json.dumps(semantic_diff_payload, indent=2) + "\n")
          debt_payload = {
              "schema_version": 1,
              "kind": "config_debt_ledger",
              "status": "warn" if (missing_owner or missing_consumer or missing_schema_or_justification or unused_schemas) else "ok",
              "missing_owner": missing_owner,
              "missing_consumer": missing_consumer,
              "missing_schema_or_justification": missing_schema_or_justification,
              "unused_schemas": unused_schemas,
          }
          (report_root / "config-debt-ledger.json").write_text(json.dumps(debt_payload, indent=2) + "\n")
          key_usage_payload = {
              "schema_version": 1,
              "kind": "config_key_usage_map",
              "status": "ok",
              "rows": key_usage,
              "registry_path": "docs/reference/contracts/schemas/CONFIG_KEYS.json",
          }
          (report_root / "config-key-usage-map.json").write_text(json.dumps(key_usage_payload, indent=2) + "\n")

          debt_total = len(missing_owner) + len(missing_consumer) + len(missing_schema_or_justification) + len(unused_schemas)
          if debt_total > 20:
              violations.append(f"config debt ledger exceeds top-20 budget: {debt_total}")

          if violations:
              raise SystemExit("configs taxonomy contracts failed: " + "; ".join(violations[:20]))
          payload = {
              "schema_version": 1,
              "kind": "configs_taxonomy_contracts",
              "status": "ok",
              "max_depth": max_depth,
              "max_files_per_dir": max_files_per_dir,
              "allowed_groups": sorted(allowed_groups),
              "checked_files": len(file_depth),
              "duplicate_tree_tails": duplicated_tails,
              "group_lifecycle": group_stability,
              "warnings": warnings,
          }
          (report_root / "configs-taxonomy-contracts.json").write_text(json.dumps(payload, indent=2) + "\n")
          PY
          cargo run -q -p bijux-dev-atlas -- configs graph --allow-write --strict --format json > "${report_root}/configs-graph.json"
          cargo run -q -p bijux-dev-atlas -- configs explain configs/inventory/configs.json --strict --format json > "${report_root}/configs-explain-configs-json.json"
          cargo run -q -p bijux-dev-atlas -- configs diff --strict --format json > "${report_root}/configs-diff.json"
          cargo run -q -p bijux-dev-atlas -- docs shrink-report --strict --format json > "${report_root}/docs-shrink-report.json"
          ! rg -n "(AKIA|BEGIN PRIVATE KEY|SECRET|TOKEN=)" "${report_root}/ci-fast.json"
          printf "- lane: ci-fast\n- report: %s\n- cache registry hit: %s\n- cache target hit: %s\n" \
            "${report_root}/ci-fast.json" "${{ steps.cache-registry.outputs.cache-hit || 'false' }}" "${{ steps.cache-target.outputs.cache-hit || 'false' }}" > "artifacts/${RUN_ID}/summary.md"
          printf "\n- contracts all: %s\n- docs required: %s\n- docs required membership: %s\n- configs required: %s\n- configs required membership: %s\n- wrappers required: %s\n- wrappers required membership: %s\n- repo required: %s\n- repo required membership: %s\n- repo integrity summary: %s\n- contracts runtime static: %s\n- contracts control-plane static: %s\n- contracts configs static: %s\n- contracts wrappers static: %s\n- contracts ops static: %s\n- contracts docker static: %s\n- coverage: %s\n" \
            "${contracts_root}/all.json" "${contracts_root}/docs-required.json" "${report_root}/suite-membership-docs_required.json" "${contracts_root}/configs-required.json" "${report_root}/suite-membership-configs_required.json" "${contracts_root}/make-required.json" "${report_root}/suite-membership-make_required.json" "${contracts_root}/repo-required.json" "${report_root}/suite-membership-repo_required.json" "${report_root}/repo-integrity-summary.json" "${contracts_root}/runtime-static.json" "${contracts_root}/control-plane-static.json" "${contracts_root}/configs-static.json" "${contracts_root}/make-static.txt" "${contracts_root}/ops-static.json" "${contracts_root}/docker-static.json" "${contracts_root}/coverage-summary.json" >> "artifacts/${RUN_ID}/summary.md"
          printf "- repo surface diff: %s\n- root change report: %s\n- governance validate: %s\n- governance coverage diff: %s\n- semantic drift summary: %s\n- wrapper surface drift: %s\n- deterministic check order: %s\n- check metadata catalog: %s\n- artifact schema check: %s\n- artifact naming and size: %s\n- docs golden workflow verification: %s\n- docs link allowlist policy: %s\n- alert runbook slo load e2e mapping: %s\n- registry group coverage: %s\n- contract coverage heatmap: %s\n- missing proof report: %s\n- proof debt ledger: %s\n- contract flake detector: %s\n- contract performance budget: %s\n- docs entrypoint and budget contracts: %s\n- docs navigation and semantics contracts: %s\n- docs quality contracts: %s\n- configs taxonomy contracts: %s\n- configs semantic diff: %s\n- config debt ledger: %s\n- config key usage map: %s\n- configs graph: %s\n- configs explain: %s\n- configs diff: %s\n- docs shrink report: %s\n" "${report_root}/repo-surface-diff.json" "${report_root}/root-change-report.json" "${report_root}/governance-validate.json" "${report_root}/governance-coverage-diff.json" "${report_root}/semantic-drift-summary.json" "${report_root}/make-surface-drift.json" "${report_root}/deterministic-check-order.json" "${report_root}/check-metadata-catalog.json" "${report_root}/artifact-schema-version-check.json" "${report_root}/artifact-naming-and-size.json" "${report_root}/docs-golden-workflow-verification.json" "${report_root}/docs-link-allowlist-policy.json" "${report_root}/ops-alert-runbook-slo-load-e2e-map.json" "${report_root}/registry-group-coverage.json" "${report_root}/contract-coverage-heatmap.json" "${report_root}/missing-proof-report.json" "${report_root}/proof-debt-ledger.json" "${report_root}/contract-flake-detector.json" "${report_root}/contract-performance-budget.json" "${report_root}/docs-entrypoint-and-budget-contracts.json" "${report_root}/docs-navigation-and-semantics-contracts.json" "${report_root}/docs-quality-contracts.json" "${report_root}/configs-taxonomy-contracts.json" "${report_root}/configs-semantic-diff.json" "${report_root}/config-debt-ledger.json" "${report_root}/config-key-usage-map.json" "${report_root}/configs-graph.json" "${report_root}/configs-explain-configs-json.json" "${report_root}/configs-diff.json" "${report_root}/docs-shrink-report.json" >> "artifacts/${RUN_ID}/summary.md"
          if [ -f "${contracts_root}/root-strict.json" ]; then
            printf -- "- root strict: %s\n" "${contracts_root}/root-strict.json" >> "artifacts/${RUN_ID}/summary.md"
          fi
          if ! git diff --quiet; then
            git status --short > "${report_root}/repo-write-drift.txt"
            jq -n --rawfile dirty "${report_root}/repo-write-drift.txt" '{
              schema_version: 1,
              kind: "repo_write_drift",
              status: "failed",
              dirty: ($dirty | split("\n") | map(select(length>0)))
            }' > "${report_root}/repo-write-drift.json"
            echo "repository changed during checks; checks must write only to artifacts" >&2
            exit 1
          fi
          jq -n '{schema_version:1,kind:"repo_write_drift",status:"ok",dirty:[]}' > "${report_root}/repo-write-drift.json"
          cat "artifacts/${RUN_ID}/summary.md" >> "${GITHUB_STEP_SUMMARY}"
      - name: Save cargo registry cache
        if: success()
        uses: actions/cache/save@0057852bfaa89a56745cba8c7296529d2fc39830
        with:
          path: |
            .cache/cargo/home/ci-pr-minimal/registry
            .cache/cargo/home/ci-pr-minimal/git
          key: cargo-registry-pr-${{ runner.os }}-1.84.1-${{ hashFiles('Cargo.lock') }}
      - name: Save cargo target cache
        if: success()
        uses: actions/cache/save@0057852bfaa89a56745cba8c7296529d2fc39830
        with:
          path: .cache/cargo/target/ci-pr-minimal
          key: cargo-target-pr-minimal-${{ runner.os }}-1.84.1-${{ hashFiles('Cargo.lock') }}
      - uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02
        if: always()
        with:
          name: ci-pr-minimal-${{ env.RUN_ID }}
          path: artifacts/${{ env.RUN_ID }}
          retention-days: 5

  validate-pr:
    if: needs.route-changes.outputs.docs_only != 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 45
    needs: route-changes
    env:
      NETWORK_ACCESS: required
      ISO_ROOT: artifacts/isolates/ci-pr
      CARGO_TARGET_DIR: .cache/cargo/target/ci-pr
      CARGO_HOME: .cache/cargo/home/ci-pr
      NEXTEST_CACHE_DIR: .cache/cargo/nextest/ci-pr
      TMPDIR: artifacts/isolates/ci-pr/tmp
      TMP: artifacts/isolates/ci-pr/tmp
      TEMP: artifacts/isolates/ci-pr/tmp
    steps:
      - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5
      - uses: dtolnay/rust-toolchain@631a55b12751854ce901bb631d5902ceb48146f7
        with:
          toolchain: 1.84.1
      - name: Restore cargo registry cache
        id: cache-registry
        uses: actions/cache/restore@0057852bfaa89a56745cba8c7296529d2fc39830
        with:
          path: |
            .cache/cargo/home/ci-pr/registry
            .cache/cargo/home/ci-pr/git
          key: cargo-registry-pr-${{ runner.os }}-1.84.1-${{ hashFiles('Cargo.lock') }}
      - name: Restore cargo target cache
        id: cache-target
        uses: actions/cache/restore@0057852bfaa89a56745cba8c7296529d2fc39830
        with:
          path: .cache/cargo/target/ci-pr
          key: cargo-target-pr-${{ runner.os }}-1.84.1-${{ hashFiles('Cargo.lock') }}
      - name: Install docs toolchain
        shell: bash
        run: |
          set -euo pipefail
          python3 -m pip install --upgrade pip
          python3 -m pip install -r configs/docs/requirements.lock.txt
      - name: Run PR lane
        shell: bash
        run: |
          set -euo pipefail
          report_root="artifacts/${RUN_ID}/reports"
          logs_root="artifacts/${RUN_ID}/logs/ci-pr"
          mkdir -p "${report_root}" "${logs_root}"
          make doctor > "${report_root}/doctor.json" 2> "${logs_root}/doctor.stderr.log"
          cargo run -q -p bijux-dev-atlas -- check tree-budgets --format json > "${report_root}/tree-budgets.json" 2> "${logs_root}/tree-budgets.stderr.log"
          make help > "${logs_root}/make-help.stdout.log" 2> "${logs_root}/make-help.stderr.log"
          make make-target-list > "${logs_root}/make-target-list.stdout.log" 2> "${logs_root}/make-target-list.stderr.log"
          cp make/target-list.json "${report_root}/make-target-list.json"
          cargo run -q -p bijux-dev-atlas -- check run --domain root --tag lint --format json > "${logs_root}/lint-policy-report.stdout.log" 2> "${logs_root}/lint-policy-report.stderr.log"
          printf 'workspace_lints_file=Cargo.toml\nclippy_conf_dir=configs/rust\n' > "${report_root}/effective-clippy-policy.txt"
          cargo run -q -p bijux-dev-atlas -- check run --domain root --tag lint --format json > "${logs_root}/lint-policy-enforce.stdout.log" 2> "${logs_root}/lint-policy-enforce.stderr.log"
          CLIPPY_CONF_DIR=configs/rust cargo clippy --workspace --all-targets --all-features --locked --message-format=json -- -D warnings > "${report_root}/clippy.json" 2> "${logs_root}/lint-clippy-json.stderr.log"
          cargo tree -p bijux-atlas-store --edges normal --no-default-features --features backend-local > "${report_root}/store-deps-backend-local.txt"
          cargo tree -p bijux-atlas-store --edges normal --no-default-features --features backend-s3 > "${report_root}/store-deps-backend-s3.txt"
          local_dep_count="$(wc -l < "${report_root}/store-deps-backend-local.txt")"
          s3_dep_count="$(wc -l < "${report_root}/store-deps-backend-s3.txt")"
          [ "${s3_dep_count}" -gt "${local_dep_count}" ]
          printf '{"schema_version":1,"kind":"store_backend_dependency_surface","backend_local_count":%s,"backend_s3_count":%s}\n' "${local_dep_count}" "${s3_dep_count}" > "${report_root}/store-deps-diff.json"
          cp ops/inventory/release-build-profile.json "${report_root}/store-release-backend-profile.json"
          cargo check -q -p bijux-atlas-store --no-default-features --features backend-local > "${logs_root}/store-backend-local-check.stdout.log" 2> "${logs_root}/store-backend-local-check.stderr.log"
          cargo check -q -p bijux-atlas-store --no-default-features --features backend-s3 > "${logs_root}/store-backend-s3-check.stdout.log" 2> "${logs_root}/store-backend-s3-check.stderr.log"
          cargo test -q -p bijux-atlas-store --no-default-features --features backend-local local_backend_roundtrip_is_hermetic -- --exact > "${logs_root}/store-backend-local-test.stdout.log" 2> "${logs_root}/store-backend-local-test.stderr.log"
          cargo test -q -p bijux-atlas-store --no-default-features --features backend-s3 http_backend_reads_from_hermetic_cached_objects -- --exact > "${logs_root}/store-backend-s3-test.stdout.log" 2> "${logs_root}/store-backend-s3-test.stderr.log"
          cargo fmt --all -- --check --config-path configs/rust/rustfmt.toml > "${logs_root}/fmt-check.stdout.log" 2> "${logs_root}/fmt-check.stderr.log"
          cargo run -q -p bijux-dev-atlas -- configs verify --format json > "${report_root}/configs-verify.json" 2> "${logs_root}/configs-verify.stderr.log"
          cargo test --workspace --locked > "${logs_root}/workspace-test.stdout.log" 2> "${logs_root}/workspace-test.stderr.log"
          cargo test -q -p bijux-atlas-api openapi_snapshot_is_deterministic_and_matches_committed_contract -- --exact > "${logs_root}/openapi-snapshot.stdout.log" 2> "${logs_root}/openapi-snapshot.stderr.log"
          cargo test -q -p bijux-atlas-api openapi_hash_matches_pinned_contract -- --exact > "${logs_root}/openapi-hash.stdout.log" 2> "${logs_root}/openapi-hash.stderr.log"
          cargo test -q -p bijux-atlas-server runtime_startup_config_contract_artifacts_match_generated > "${logs_root}/runtime-startup-config-contract.stdout.log" 2> "${logs_root}/runtime-startup-config-contract.stderr.log"
          mkdocs build --strict > "${report_root}/mkdocs-build.json" 2> "${logs_root}/mkdocs-build.stderr.log"
          cp -R site "artifacts/${RUN_ID}/site-preview"
          cargo run -q -p bijux-dev-atlas -- docs external-links --allow-network --format json > "${report_root}/docs-linkcheck.json" 2> "${logs_root}/docs-linkcheck.stderr.log"
          ARTIFACT_ROOT="${report_root}" RUN_ID="${RUN_ID}" make docker-contracts > "${logs_root}/docker-contracts-static.stdout.log" 2> "${logs_root}/docker-contracts-static.stderr.log"
          make ops-contracts > "${logs_root}/ops-contracts-static.stdout.log" 2> "${logs_root}/ops-contracts-static.stderr.log"
          make ops-pr > "${report_root}/ci-pr.json" 2> "${logs_root}/ci-pr.stderr.log"
          cargo run -q -p bijux-dev-atlas -- check list --suite ci_pr --include-internal --include-slow --format json > "${report_root}/suite-membership-ci_pr.json"
          ! rg -n "(AKIA|BEGIN PRIVATE KEY|SECRET|TOKEN=)" "${report_root}/ci-pr.json"
          printf "- lane: ci-pr\n- report: %s\n- docs build: %s\n- docs linkcheck: %s\n- docs preview: %s\n- cache registry hit: %s\n- cache target hit: %s\n" \
            "${report_root}/ci-pr.json" "${report_root}/mkdocs-build.json" "${report_root}/docs-linkcheck.json" "artifacts/${RUN_ID}/site-preview" "${{ steps.cache-registry.outputs.cache-hit || 'false' }}" "${{ steps.cache-target.outputs.cache-hit || 'false' }}" > "artifacts/${RUN_ID}/summary.md"
          cat "artifacts/${RUN_ID}/summary.md" >> "${GITHUB_STEP_SUMMARY}"
      - name: Save cargo registry cache
        if: success()
        uses: actions/cache/save@0057852bfaa89a56745cba8c7296529d2fc39830
        with:
          path: |
            .cache/cargo/home/ci-pr/registry
            .cache/cargo/home/ci-pr/git
          key: cargo-registry-pr-${{ runner.os }}-1.84.1-${{ hashFiles('Cargo.lock') }}
      - name: Save cargo target cache
        if: success()
        uses: actions/cache/save@0057852bfaa89a56745cba8c7296529d2fc39830
        with:
          path: .cache/cargo/target/ci-pr
          key: cargo-target-pr-${{ runner.os }}-1.84.1-${{ hashFiles('Cargo.lock') }}
      - uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02
        if: always()
        with:
          name: ci-pr-${{ env.RUN_ID }}
          path: artifacts/${{ env.RUN_ID }}
          retention-days: 5

  supply-chain:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    env:
      NETWORK_ACCESS: required
      ISO_ROOT: artifacts/isolates/ci-supply-chain
      CARGO_TARGET_DIR: .cache/cargo/target/ci-supply-chain
      CARGO_HOME: .cache/cargo/home/ci-supply-chain
      NEXTEST_CACHE_DIR: .cache/cargo/nextest/ci-supply-chain
      TMPDIR: artifacts/isolates/ci-supply-chain/tmp
      TMP: artifacts/isolates/ci-supply-chain/tmp
      TEMP: artifacts/isolates/ci-supply-chain/tmp
    steps:
      - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5
      - uses: dtolnay/rust-toolchain@631a55b12751854ce901bb631d5902ceb48146f7
        with:
          toolchain: 1.84.1
      - name: Restore cargo registry cache
        id: cache-registry
        uses: actions/cache/restore@0057852bfaa89a56745cba8c7296529d2fc39830
        with:
          path: |
            .cache/cargo/home/ci-supply-chain/registry
            .cache/cargo/home/ci-supply-chain/git
          key: cargo-registry-pr-${{ runner.os }}-1.84.1-${{ hashFiles('Cargo.lock') }}
      - name: Restore cargo target cache
        id: cache-target
        uses: actions/cache/restore@0057852bfaa89a56745cba8c7296529d2fc39830
        with:
          path: .cache/cargo/target/ci-supply-chain
          key: cargo-target-pr-supply-${{ runner.os }}-1.84.1-${{ hashFiles('Cargo.lock') }}
      - name: Run audit lane
        shell: bash
        run: |
          set -euo pipefail
          report_root="artifacts/${RUN_ID}/reports"
          logs_root="artifacts/${RUN_ID}/logs/ci-supply-chain"
          mkdir -p "${report_root}" "${logs_root}"
          cargo install --locked cargo-deny >/dev/null 2> "${logs_root}/cargo-deny-install.stderr.log"
          cargo deny --config configs/security/deny.toml check > "${logs_root}/deny.stdout.log" 2> "${logs_root}/deny.stderr.log"
          cargo audit > "${logs_root}/audit.stdout.log" 2> "${logs_root}/audit.stderr.log"
          printf '{"schema_version":1,"kind":"supply_chain_report_v1","commands":["cargo deny --config configs/security/deny.toml check","cargo audit"],"status":"ok"}\n' > "${report_root}/supply-chain.json"
          ! rg -n "(AKIA|BEGIN PRIVATE KEY|SECRET|TOKEN=)" "${report_root}/supply-chain.json"
          printf "- lane: supply-chain\n- report: %s\n- cache registry hit: %s\n- cache target hit: %s\n" \
            "${report_root}/supply-chain.json" "${{ steps.cache-registry.outputs.cache-hit || 'false' }}" "${{ steps.cache-target.outputs.cache-hit || 'false' }}" > "artifacts/${RUN_ID}/summary.md"
          cat "artifacts/${RUN_ID}/summary.md" >> "${GITHUB_STEP_SUMMARY}"
      - name: Save cargo registry cache
        if: success()
        uses: actions/cache/save@0057852bfaa89a56745cba8c7296529d2fc39830
        with:
          path: |
            .cache/cargo/home/ci-supply-chain/registry
            .cache/cargo/home/ci-supply-chain/git
          key: cargo-registry-pr-${{ runner.os }}-1.84.1-${{ hashFiles('Cargo.lock') }}
      - name: Save cargo target cache
        if: success()
        uses: actions/cache/save@0057852bfaa89a56745cba8c7296529d2fc39830
        with:
          path: .cache/cargo/target/ci-supply-chain
          key: cargo-target-pr-supply-${{ runner.os }}-1.84.1-${{ hashFiles('Cargo.lock') }}
      - uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02
        if: always()
        with:
          name: ci-supply-chain-${{ env.RUN_ID }}
          path: artifacts/${{ env.RUN_ID }}
          retention-days: 5

  workflow-policy:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5
      - name: Check action pinning policy
        shell: bash
        run: |
          set -euo pipefail
          violations="$(rg -n "uses:\s+[^@\s]+@(main|master|HEAD|latest)$" .github/workflows || true)"
          if [ -n "${violations}" ]; then
            echo "unacceptable floating action references found"
            echo "${violations}"
            exit 1
          fi
      - name: Enforce no direct script execution in workflows
        shell: bash
        run: |
          set -euo pipefail
          ! rg -n "bash +scripts/|python(3)? +scripts/" .github/workflows
      - name: Enforce no direct external tool invocations
        shell: bash
        run: |
          set -euo pipefail
          ! rg -n "(^|\s)(helm|kubectl|k6|kind)(\s|$)" .github/workflows \
            -g '!ops-integration-kind.yml' -g '!ci-pr.yml'
      - name: Enforce dependabot config contract
        shell: bash
        run: |
          set -euo pipefail
          test -f .github/dependabot.yml
          rg -n 'package-ecosystem:\s+"cargo"' .github/dependabot.yml
          rg -n 'package-ecosystem:\s+"github-actions"' .github/dependabot.yml
      - name: Enforce docs ownership contract
        shell: bash
        run: |
          set -euo pipefail
          test -f .github/CODEOWNERS
          rg -n '^/docs/ +@bijan' .github/CODEOWNERS
          rg -n '^/crates/\*/docs/ +@bijan' .github/CODEOWNERS
      - name: Enforce cache size cap
        shell: bash
        run: |
          set -euo pipefail
          if [ -d .cache/cargo/target ]; then
            size_kb="$(du -sk .cache/cargo/target | awk '{print $1}')"
            [ "${size_kb}" -le 4194304 ]
          fi

  docker-runtime-image:
    if: needs.route-changes.outputs.docs_only != 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 25
    needs: route-changes
    env:
      ISO_ROOT: artifacts/isolates/ci-pr-docker
      TMPDIR: artifacts/isolates/ci-pr-docker/tmp
      TMP: artifacts/isolates/ci-pr-docker/tmp
      TEMP: artifacts/isolates/ci-pr-docker/tmp
    steps:
      - uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5
      - uses: dtolnay/rust-toolchain@631a55b12751854ce901bb631d5902ceb48146f7
        with:
          toolchain: 1.84.1
      - name: Run docker gate lane
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p "artifacts/${RUN_ID}"
          make docker-contracts > "artifacts/${RUN_ID}/docker-contracts.stdout.log" 2> "artifacts/${RUN_ID}/docker-contracts.stderr.log"
      - uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02
        if: always()
        with:
          name: docker-runtime-${{ env.RUN_ID }}
          path: artifacts/${{ env.RUN_ID }}
          retention-days: 5
